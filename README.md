# 1.Transfer Learning 

æ¨èä½ é˜…è¯»ä»¥ä¸‹ææ–™æ¥åŠ æ·±å¯¹ CNNå’ŒTransfer Learningçš„ç†è§£:

**[CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)**

> ç‚¹è¿™é‡ŒæŸ¥çœ‹[ç¬”è®°](https://github.com/daxingxingqi/CS231n-2017-Summary)

> ç‚¹è¿™é‡ŒæŸ¥çœ‹å®˜æ–¹[ç¬”è®°](https://cs231n.github.io/)

**[Using Convolutional Neural Networks to Classify Dog Breeds](http://cs231n.stanford.edu/reports/2015/pdfs/fcdh_FinalReport.pdf)**

**[Building an Image Classifier](https://towardsdatascience.com/learning-about-data-science-building-an-image-classifier-part-2-a7bcc6d5e825)**

**[Tips/Tricks in CNN](http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html)**

- 1) data augmentation; 
- 2) pre-processing on images; 
- 3) initializations of Networks; 
- 4) some tips during training; 
- 5) selections of activation functions; 
- 6) diverse regularizations; 
- 7) some insights found from figures and finally 
- 8) methods of ensemble multiple deep networks.

## [Transfer Learning using Keras](https://towardsdatascience.com/transfer-learning-using-keras-d804b2e04ef8)

> 1. **New dataset is small and similar to original dataset:**
>    There is a problem of over-fitting, if we try to train the entire network. Since the data is similar to the original data, we expect higher-level features in the ConvNet to be relevant to this dataset as well. Hence, the best idea might be to train a linear classifier on the CNN codes.

So lets freeze all the VGG19 layers and train only the classifier

```python
for layer in model.layers:
   layer.trainable = False
 
#Now we will be training only the classifiers (FC layers)
```

> 1. **New dataset is large and similar to the original dataset**
>    Since we have more data, we can have more confidence that we wonâ€™t overfit if we were to try to fine-tune through the full network.

```python
for layer in model.layers:
   layer.trainable = True
#The default is already set to True. I have mentioned it here to make things clear.
```

In case if you want to freeze the first few layers as these layers will be detecting edges and blobs, you can freeze them by using the following code.

```python
for layer in model.layers[:5]:
   layer.trainable = False.
# Here I am freezing the first 5 layers 
```

> 1. **New dataset is small but very different from the original dataset**
>    Since the dataset is very small, We may want to extract the features from the earlier layer and train a classifier on top of that. This requires a little bit of knowledge on h5py.The above code should help. It will extract the â€œblock2_poolâ€ features. In general this is not helpful as this layer has (64*64*128) features and training a classifier on top of it might not help us exactly. We can add a few FC layers and train a neural network on top of it. That should be straight forward.

> - Add few FC layers and output layer.

> - Set the weights for earlier layers and freeze them.

> - Train the network.

> 1. **New dataset is large and very different from the original dataset.**
>    This is straight forward. since you have large dataset, you can design your own network or use the existing ones.

> - Train the network using random initialisations or use the pre-trained network weights as initialisers. The second one is generally preferred.

> - If you are using a different network or making small modification here and there for the existing network, Be careful with the naming conventions.

[Transfer Learning in TensorFlow on the Kaggle Rainforest competition](https://medium.com/@luckylwk/transfer-learning-in-tensorflow-on-the-kaggle-rainforest-competition-4e978fadb571)

[Transfer Learning and Fine-tuning](https://medium.com/deeplearningsandbox/how-to-use-transfer-learning-and-fine-tuning-in-keras-and-tensorflow-to-build-an-image-recognition-94b0b02444f2)

# 2.æ¡†æ¶è®ºæ–‡

**[[VGG16] VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION](https://arxiv.org/abs/1409.1556)**

- VGG16

```python
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout, Flatten
from keras.layers import Conv2D
from keras.layers import MaxPooling2D

input_shape = (224, 224, 3)

model = Sequential([
    Conv2D(64, (3, 3), input_shape=input_shape, padding='same',
           activation='relu'),
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(128, (3, 3), activation='relu', padding='same'),
    Conv2D(128, (3, 3), activation='relu', padding='same',),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Flatten(),
    Dense(4096, activation='relu'),
    Dense(4096, activation='relu'),
    Dense(1000, activation='softmax')
])

model.summary()
```

- VGG19

```python
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout, Flatten
from keras.layers import Conv2D
from keras.layers import MaxPooling2D

input_shape = (224, 224, 3)

model = Sequential([
    Conv2D(64, (3, 3), input_shape=input_shape, padding='same',
           activation='relu'),
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(128, (3, 3), activation='relu', padding='same'),
    Conv2D(128, (3, 3), activation='relu', padding='same',),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    Conv2D(256, (3, 3), activation='relu', padding='same',)
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',)
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',)
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Flatten(),
    Dense(4096, activation='relu'),
    Dense(4096, activation='relu'),
    Dense(1000, activation='softmax')
])

model.summary()
```

**[[Inception-v1] Going deeper with convolutions](https://arxiv.org/abs/1409.4842)**

ä¸ºä»€ä¹ˆ1*1å·ç§¯-https://iamaaditya.github.io/2016/03/one-by-one-convolution/

è®²è§£-https://becominghuman.ai/understanding-and-coding-inception-module-in-keras-eb56e9056b4b

<div align=center><img width="550" src=resource/1.png></div>

```python
#-*- coding: UTF-8 -*-
"""
Author: lanbing510
Environment: Keras2.0.5ï¼ŒPython2.7
Model: GoogLeNet Inception V1
"""

from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D
from keras.layers import Flatten, Dense, Dropout
from keras.layers import Input, concatenate
from keras.models import Model
from keras import regularizers
from keras.utils import plot_model
from KerasLayers.Custom_layers import LRN2D


# Global Constants
NB_CLASS=1000
LEARNING_RATE=0.01
MOMENTUM=0.9
ALPHA=0.0001
BETA=0.75
GAMMA=0.1
DROPOUT=0.4
WEIGHT_DECAY=0.0005
LRN2D_NORM=True
DATA_FORMAT='channels_last' # Theano:'channels_first' Tensorflow:'channels_last'
USE_BN=True


def conv2D_lrn2d(x,filters,kernel_size,strides=(1,1),padding='same',data_format=DATA_FORMAT,dilation_rate=(1,1),activation='relu',use_bias=True,kernel_initializer='glorot_uniform',bias_initializer='zeros',kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,lrn2d_norm=LRN2D_NORM,weight_decay=WEIGHT_DECAY):
    if weight_decay:
        kernel_regularizer=regularizers.l2(weight_decay)
        bias_regularizer=regularizers.l2(weight_decay)
    else:
        kernel_regularizer=None
        bias_regularizer=None

    x=Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)

    if lrn2d_norm:
        x=LRN2D(alpha=ALPHA,beta=BETA)(x)

    return x



def inception_module(x,params,concat_axis,padding='same',data_format=DATA_FORMAT,dilation_rate=(1,1),activation='relu',use_bias=True,kernel_initializer='glorot_uniform',bias_initializer='zeros',kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,lrn2d_norm=LRN2D_NORM,weight_decay=None):
    (branch1,branch2,branch3,branch4)=params
    if weight_decay:
        kernel_regularizer=regularizers.l2(weight_decay)
        bias_regularizer=regularizers.l2(weight_decay)
    else:
        kernel_regularizer=None
        bias_regularizer=None

    pathway1=Conv2D(filters=branch1[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)

    pathway2=Conv2D(filters=branch2[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)
    pathway2=Conv2D(filters=branch2[1],kernel_size=(3,3),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway2)

    pathway3=Conv2D(filters=branch3[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)
    pathway3=Conv2D(filters=branch3[1],kernel_size=(5,5),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway3)

    pathway4=MaxPooling2D(pool_size=(3,3),strides=1,padding=padding,data_format=DATA_FORMAT)(x)
    pathway4=Conv2D(filters=branch4[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway4)

    return concatenate([pathway1,pathway2,pathway3,pathway4],axis=concat_axis)



def create_model():
    if DATA_FORMAT=='channels_first':
        INP_SHAPE=(3,224,224)
        img_input=Input(shape=INP_SHAPE)
        CONCAT_AXIS=1
    elif DATA_FORMAT=='channels_last':
        INP_SHAPE=(224,224,3)
        img_input=Input(shape=INP_SHAPE)
        CONCAT_AXIS=3
    else:
        raise Exception('Invalid Dim Ordering: '+str(DIM_ORDERING))

    x=conv2D_lrn2d(img_input,64,(7,7),2,padding='same',lrn2d_norm=False)
    x=MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x)
    x=LRN2D(alpha=ALPHA,beta=BETA)(x)

    x=conv2D_lrn2d(x,64,(1,1),1,padding='same',lrn2d_norm=False)

    x=conv2D_lrn2d(x,192,(3,3),1,padding='same',lrn2d_norm=True)
    x=MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x)

    x=inception_module(x,params=[(64,),(96,128),(16,32),(32,)],concat_axis=CONCAT_AXIS) #3a
    x=inception_module(x,params=[(128,),(128,192),(32,96),(64,)],concat_axis=CONCAT_AXIS) #3b
    x=MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x)

    x=inception_module(x,params=[(192,),(96,208),(16,48),(64,)],concat_axis=CONCAT_AXIS) #4a
    x=inception_module(x,params=[(160,),(112,224),(24,64),(64,)],concat_axis=CONCAT_AXIS) #4b
    x=inception_module(x,params=[(128,),(128,256),(24,64),(64,)],concat_axis=CONCAT_AXIS) #4c
    x=inception_module(x,params=[(112,),(144,288),(32,64),(64,)],concat_axis=CONCAT_AXIS) #4d
    x=inception_module(x,params=[(256,),(160,320),(32,128),(128,)],concat_axis=CONCAT_AXIS) #4e
    x=MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x)

    x=inception_module(x,params=[(256,),(160,320),(32,128),(128,)],concat_axis=CONCAT_AXIS) #5a
    x=inception_module(x,params=[(384,),(192,384),(48,128),(128,)],concat_axis=CONCAT_AXIS) #5b
    x=AveragePooling2D(pool_size=(7,7),strides=1,padding='valid',data_format=DATA_FORMAT)(x)

    x=Flatten()(x)
    x=Dropout(DROPOUT)(x)
    x=Dense(output_dim=NB_CLASS,activation='linear')(x)
    x=Dense(output_dim=NB_CLASS,activation='softmax')(x)

    return x,img_input,CONCAT_AXIS,INP_SHAPE,DATA_FORMAT


def check_print():
    # Create the Model
    x,img_input,CONCAT_AXIS,INP_SHAPE,DATA_FORMAT=create_model()

    # Create a Keras Model
    model=Model(input=img_input,output=[x])
    model.summary()

    # Save a PNG of the Model Build
    plot_model(model,to_file='GoogLeNet.png')

    model.compile(optimizer='rmsprop',loss='categorical_crossentropy')
    print 'Model Compiled'


if __name__=='__main__':
    check_print() 
```

 **[[Inception V2], Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://arxiv.org/abs/1502.03167)**

Inception V2çš„ç½‘ç»œåœ¨Inception v1çš„åŸºç¡€ä¸Šï¼Œè¿›è¡Œäº†æ”¹è¿›ï¼Œä¸€æ–¹é¢äº†åŠ å…¥äº†BNå±‚ï¼Œå‡å°‘äº†Internal Covariate Shiftï¼ˆå†…éƒ¨ç¥ç»å…ƒåˆ†å¸ƒçš„æ”¹å˜ï¼‰ï¼Œä½¿æ¯ä¸€å±‚çš„è¾“å‡ºéƒ½è§„èŒƒåŒ–åˆ°ä¸€ä¸ªN(0, 1)çš„é«˜æ–¯ï¼Œè¿˜å»é™¤äº†Dropoutã€LRNç­‰ç»“æ„ï¼›å¦å¤–ä¸€æ–¹é¢å­¦ä¹ VGGç”¨2ä¸ª3x3çš„å·ç§¯æ›¿ä»£inceptionæ¨¡å—ä¸­çš„5x5å·ç§¯ï¼Œæ—¢é™ä½äº†å‚æ•°æ•°é‡ï¼ŒåˆåŠ é€Ÿè®¡ç®—ã€‚

**[[Inception-v3] Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567)**

 Inception V3ä¸€ä¸ªæœ€é‡è¦çš„æ”¹è¿›æ˜¯åˆ†è§£ï¼ˆFactorizationï¼‰ï¼Œå°†7x7åˆ†è§£æˆä¸¤ä¸ªä¸€ç»´çš„å·ç§¯ï¼ˆ1x7,7x1ï¼‰ï¼Œ3x3ä¹Ÿæ˜¯ä¸€æ ·ï¼ˆ1x3,3x1ï¼‰ã€‚è¿™æ ·çš„å¥½å¤„ï¼Œæ—¢å¯ä»¥åŠ é€Ÿè®¡ç®—ï¼ˆå¤šä½™çš„è®¡ç®—èƒ½åŠ›å¯ä»¥ç”¨æ¥åŠ æ·±ç½‘ç»œï¼‰ï¼Œåˆå¯ä»¥å°†1ä¸ªconvæ‹†æˆ2ä¸ªconvï¼Œä½¿å¾—ç½‘ç»œæ·±åº¦è¿›ä¸€æ­¥å¢åŠ ï¼Œå¢åŠ äº†ç½‘ç»œçš„éçº¿æ€§ï¼Œå¯ä»¥å¤„ç†æ›´å¤šæ›´ä¸°å¯Œçš„ç©ºé—´ç‰¹å¾ï¼Œå¢åŠ ç‰¹å¾å¤šæ ·æ€§ã€‚è¿˜æœ‰å€¼å¾—æ³¨æ„çš„åœ°æ–¹æ˜¯ç½‘ç»œè¾“å…¥ä»224x224å˜ä¸ºäº†299x299ï¼Œæ›´åŠ ç²¾ç»†è®¾è®¡äº†35x35/17x17/8x8çš„æ¨¡å—ã€‚

**[[Inception-v4] Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/abs/1602.07261)**

Inception V4ç»“åˆäº†å¾®è½¯çš„ResNetï¼Œå‘ç°ResNetçš„ç»“æ„å¯ä»¥æå¤§åœ°åŠ é€Ÿè®­ç»ƒï¼ŒåŒæ—¶æ€§èƒ½ä¹Ÿæœ‰æå‡ï¼Œå¾—åˆ°ä¸€ä¸ªInception-ResNet V2ç½‘ç»œï¼ŒåŒæ—¶è¿˜è®¾è®¡äº†ä¸€ä¸ªæ›´æ·±æ›´ä¼˜åŒ–çš„Inception V4æ¨¡å‹ï¼Œèƒ½è¾¾åˆ°ä¸Inception-ResNet V2ç›¸åª²ç¾çš„æ€§èƒ½ã€‚

**[[ResNet] Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)**

è®²è§£-https://blog.waya.ai/deep-residual-learning-9610bb62c355

**[[Xception] Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357)**

è®²è§£-https://blog.csdn.net/u014380165/article/details/75142710

# æŸå¤±å’Œä¼˜åŒ–ç®—æ³•

*æŸå¤±å‡½æ•°æ˜¯ç”¨æ¥ä¼°é‡æ¨¡å‹ä¸­é¢„æµ‹å€¼yä¸çœŸå®å€¼Yä¹‹é—´çš„å·®å¼‚ï¼Œå³ä¸ä¸€è‡´ç¨‹åº¦

å¦‚æœä½ æƒ³è¯¦ç»†äº†è§£ Keras ä¸­çš„å®Œå…¨è¿æ¥å±‚ï¼Œè¯·é˜…è¯»è¿™ç¯‡å…³äºå¯†é›†å±‚çš„[æ–‡æ¡£](https://keras.io/layers/core/)ã€‚ä½ å¯ä»¥é€šè¿‡ä¸º **kernel_initializer** å’Œ **bias_initializer** å‚æ•°æä¾›å€¼æ›´æ”¹æƒé‡çš„åˆå§‹åŒ–æ–¹æ³•ã€‚æ³¨æ„é»˜è®¤å€¼åˆ†åˆ«ä¸º **'glorot_uniform'** å’Œ **'zeros'**ã€‚ä½ å¯ä»¥åœ¨ç›¸åº”çš„ Keras [æ–‡æ¡£](https://keras.io/initializers/)ä¸­è¯¦ç»†äº†è§£æ¯ç§åˆå§‹åŒ–ç¨‹åºçš„å·¥ä½œæ–¹æ³•ã€‚

Keras ä¸­æœ‰å¾ˆå¤šä¸åŒçš„[æŸå¤±å‡½æ•°](https://keras.io/losses/)ã€‚å¯¹äºè¿™èŠ‚è¯¾æ¥è¯´ï¼Œæˆ‘ä»¬å°†ä»…ä½¿ç”¨ **categorical_crossentropy**ã€‚

å‚é˜… Keras ä¸­å¯[ç”¨çš„ä¼˜åŒ–ç¨‹åºåˆ—è¡¨](https://keras.io/optimizers/)ã€‚å½“ä½ ç¼–è¯‘æ¨¡å‹ï¼ˆåœ¨è®°äº‹æœ¬çš„ç¬¬ 7 æ­¥ï¼‰æ—¶å°±ä¼šæŒ‡å®šä¼˜åŒ–ç¨‹åºã€‚

> **'sgd'** : SGD

> **'rmsprop'** : RMSprop

> **'adagrad'** : Adagrad

> **'adadelta'** : Adadelta

> **'adam'** : Adam

> **'adamax'** : Adamax

> **'nadam'** : Nadam

> **'tfoptimizer'** : TFOptimizer

**å…³äºæ¿€æ´»å‡½æ•°çš„[æ–‡æ¡£](http://cs231n.github.io/neural-networks-1/#actfun)**

# checkpoint

*åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨å¾ˆå¤šå›è°ƒï¼ˆä¾‹å¦‚ ModelCheckpointï¼‰æ¥ç›‘æ§ä½ çš„æ¨¡å‹ã€‚ä½ å¯ä»¥å‚é˜…æ­¤å¤„çš„[è¯¦æƒ…å†…å®¹](https://keras.io/callbacks/#modelcheckpoint)ã€‚å»ºè®®ä½ å…ˆè¯¦ç»†äº†è§£ EarlyStopping å›è°ƒã€‚å¦‚æœä½ æƒ³æŸ¥çœ‹å¦ä¸€ä¸ª ModelCheckpoint ä»£ç ç¤ºä¾‹ï¼Œè¯·å‚é˜…[è¿™ç¯‡åšæ–‡](http://machinelearningmastery.com/check-point-deep-learning-models-keras/)ã€‚

*Mnist å‚é˜…[å…¶ä»–åˆ†ç±»å™¨](http://yann.lecun.com/exdb/mnist/)çš„æ•ˆæœ

# æ± åŒ–

è¯·å‚é˜…è¯¥ Keras [æ–‡æ¡£](https://keras.io/layers/pooling/)ï¼Œäº†è§£ä¸åŒç±»å‹çš„æ± åŒ–å±‚ï¼

è®ºæ–‡[network in network](https://arxiv.org/abs/1312.4400)

å‚é˜… CIFAR-10 ç«èµ›çš„[è·èƒœæ¶æ„](http://blog.kaggle.com/2015/01/02/cifar-10-competition-winners-interviews-with-dr-ben-graham-phil-culliton-zygmunt-zajac/)ï¼

# æ•°æ®å¢å¼º

å…³äº `steps_per_epoch` çš„æ³¨æ„äº‹é¡¹

`fit_generator` å…·æœ‰å¾ˆå¤šå‚æ•°ï¼ŒåŒ…æ‹¬

```python
steps_per_epoch = x_train.shape[0] / batch_size
```

å…¶ä¸­ `x_train.shape[0]` å¯¹åº”çš„æ˜¯è®­ç»ƒæ•°æ®é›† x_train ä¸­çš„ç‹¬ç‰¹æ ·æœ¬æ•°é‡ã€‚é€šè¿‡å°† steps_per_epoch è®¾ä¸ºæ­¤å€¼ï¼Œæˆ‘ä»¬ç¡®ä¿æ¨¡å‹åœ¨æ¯ä¸ª epoch ä¸­çœ‹åˆ° `x_train.shape[0]` ä¸ªå¢å¼ºå›¾ç‰‡ã€‚

> é˜…è¯»è¿™ç¯‡å¯¹ MNIST æ•°æ®é›†è¿›è¡Œå¯è§†åŒ–çš„[ç²¾å½©åšæ–‡](http://machinelearningmastery.com/image-augmentation-deep-learning-keras/)ã€‚

> å‚é˜…æ­¤[è¯¦ç»†å®ç°](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨å¢å¼ºåŠŸèƒ½æé«˜ Kaggle æ•°æ®é›†çš„æ•ˆæœã€‚

> é˜…è¯»å…³äº ImageDataGenerator ç±»çš„ Keras [æ–‡æ¡£](https://keras.io/preprocessing/image/)ã€‚

# é˜²æ­¢è¿‡æ‹Ÿåˆ

# è¡¥å……èµ„æ–™

å‚é˜… [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) è®ºæ–‡ï¼

åœ¨æ­¤å¤„è¯¦ç»†äº†è§£ [VGGNet](https://arxiv.org/pdf/1409.1556.pdf)ã€‚

æ­¤å¤„æ˜¯ [ResNet](https://arxiv.org/pdf/1512.03385v1.pdf) è®ºæ–‡ã€‚

è¿™æ˜¯ç”¨äºè®¿é—®ä¸€äº›è‘—å CNN æ¶æ„çš„ Keras [æ–‡æ¡£](https://keras.io/applications/)ã€‚

é˜…è¯»è¿™ä¸€å…³äºæ¢¯åº¦æ¶ˆå¤±é—®é¢˜çš„[è¯¦ç»†å¤„ç†æ–¹æ¡ˆ](http://neuralnetworksanddeeplearning.com/chap5.html)ã€‚

è¿™æ˜¯åŒ…å«ä¸åŒ CNN æ¶æ„çš„åŸºå‡†çš„ GitHub [èµ„æºåº“](https://github.com/jcjohnson/cnn-benchmarks)ã€‚

è®¿é—® [ImageNet Large Scale Visual Recognition Competition (ILSVRC)](http://www.image-net.org/challenges/LSVRC/) ç½‘ç«™ã€‚

å¯ä»¥åœ¨[æ­¤å¤„](https://github.com/udacity/machine-learning/tree/master/projects/practice_projects/cnn)é“¾æ¥çš„ GitHub èµ„æºåº“ä¸­è®¿é—®è§†é¢‘ä¸­æåˆ°çš„ Jupyter Notebookã€‚è½¬åˆ° transfer-learning/ æ–‡ä»¶å¤¹å¹¶æ‰“å¼€ transfer_learning.ipynbã€‚å¦‚æœä½ æƒ³äº†è§£å¦‚ä½•è®¡ç®—è‡ªå·±çš„ç“¶é¢ˆç‰¹å¾ï¼Œè¯·æŸ¥çœ‹ bottleneck_features.ipynbï¼ˆä½ å¯èƒ½æ— æ³•åœ¨ AWS GPU å®ä¾‹ä¸Šè¿è¡Œ bottleneck_features.ipynbï¼Œå¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œä½ å¯ä»¥åœ¨æœ¬åœ° CPU/GPU ä¸Šä½¿ç”¨ notebookï¼ï¼‰

è¯¾å¤–èµ„æ–™
è¿™æ˜¯æè®®å°† GAP å±‚çº§ç”¨äºå¯¹è±¡å®šä½çš„[é¦–ç¯‡ç ”ç©¶è®ºæ–‡](http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf)ã€‚
å‚é˜…è¿™ä¸ªä½¿ç”¨ CNN è¿›è¡Œå¯¹è±¡å®šä½çš„[èµ„æºåº“](https://github.com/alexisbcook/ResNetCAM-keras)ã€‚
è§‚çœ‹è¿™ä¸ªå…³äºä½¿ç”¨ CNN è¿›è¡Œå¯¹è±¡å®šä½çš„[è§†é¢‘æ¼”ç¤º](https://www.youtube.com/watch?v=fZvOy0VXWAI)(Youtubeé“¾æ¥ï¼Œå›½å†…ç½‘ç»œå¯èƒ½æ‰“ä¸å¼€)ã€‚
å‚é˜…è¿™ä¸ªä½¿ç”¨å¯è§†åŒ–æœºå™¨æ›´å¥½åœ°ç†è§£ç“¶é¢ˆç‰¹å¾çš„[èµ„æºåº“](https://github.com/alexisbcook/keras_transfer_cifar10)ã€‚

ï¼ˆéå¸¸æ£’çš„ï¼‰è¯¾å¤–èµ„æ–™ ï¼

æ³¨ï¼šç”±äºä»¥ä¸‹éƒ¨åˆ†é“¾æ¥æ¥è‡ªäºå¤–ç½‘ï¼Œå›½å†…ç½‘ç»œå¯èƒ½æ‰“ä¸å¼€

å¦‚æœä½ æƒ³è¯¦ç»†äº†è§£å¦‚ä½•è§£è¯» CNNï¼ˆå°¤å…¶æ˜¯å·ç§¯å±‚ï¼‰ï¼Œå»ºè®®æŸ¥çœ‹ä»¥ä¸‹èµ„æ–™ï¼š

> è¿™æ˜¯æ‘˜è‡ªæ–¯å¦ç¦å¤§å­¦çš„ CS231n è¯¾ç¨‹ä¸­çš„ä¸€ä¸ªa [ç« èŠ‚](http://cs231n.github.io/understanding-cnn/)ï¼Œå…¶ä¸­å¯¹ CNN å­¦ä¹ çš„å†…å®¹è¿›è¡Œäº†å¯è§†åŒ–ã€‚

> å‚é˜…è¿™ä¸ªå…³äºå¾ˆé…·çš„ [OpenFrameworks](http://openframeworks.cc/) åº”ç”¨çš„[æ¼”ç¤º](https://aiexperiments.withgoogle.com/what-neural-nets-see)ï¼Œè¯¥åº”ç”¨å¯ä»¥æ ¹æ®ç”¨æˆ·æä¾›çš„è§†é¢‘å®æ—¶å¯è§†åŒ– CNNï¼

> è¿™æ˜¯å¦ä¸€ä¸ª CNN å¯è§†åŒ–å·¥å…·çš„[æ¼”ç¤º](https://www.youtube.com/watch?v=AgkfIQ4IGaM&t=78s)ã€‚å¦‚æœä½ æƒ³è¯¦ç»†äº†è§£è¿™äº›å¯è§†åŒ–å›¾è¡¨æ˜¯å¦‚ä½•åˆ¶ä½œçš„ï¼Œè¯·è§‚çœ‹æ­¤[è§†é¢‘](https://www.youtube.com/watch?v=ghEmQSxT6tw&t=5s)ã€‚

> è¿™æ˜¯å¦ä¸€ä¸ªå¯ä¸ Keras å’Œ Tensorflow ä¸­çš„ CNN æ— ç¼åˆä½œçš„[å¯è§†åŒ–å·¥å…·](https://medium.com/merantix/picasso-a-free-open-source-visualizer-for-cnns-d8ed3a35cfc5)ã€‚

> é˜…è¯»è¿™ç¯‡å¯è§†åŒ– CNN å¦‚ä½•çœ‹å¾…è¿™ä¸ªä¸–ç•Œçš„ [Keras åšæ–‡](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)ã€‚åœ¨æ­¤åšæ–‡ä¸­ï¼Œä½ ä¼šæ‰¾åˆ° Deep Dreams çš„ç®€å•ä»‹ç»ï¼Œä»¥åŠåœ¨ Keras ä¸­è‡ªå·±ç¼–å†™ Deep Dreams çš„ä»£ç ã€‚é˜…è¯»äº†è¿™ç¯‡åšæ–‡åï¼š

> å†è§‚çœ‹è¿™ä¸ªåˆ©ç”¨ [Deep Dreams](https://www.youtube.com/watch?v=XatXy6ZhKZw) çš„éŸ³ä¹è§†é¢‘ï¼ˆæ³¨æ„ 3:15-3:40 éƒ¨åˆ†ï¼‰ï¼

> ä½¿ç”¨è¿™ä¸ª[ç½‘ç«™](https://deepdreamgenerator.com/)åˆ›å»ºè‡ªå·±çš„ Deep Dreamsï¼ˆä¸ç”¨ç¼–å†™ä»»ä½•ä»£ç ï¼ï¼‰ã€‚

å¦‚æœä½ æƒ³è¯¦ç»†äº†è§£ CNN çš„è§£é‡Š

> è¿™ç¯‡[æ–‡ç« ](https://blog.openai.com/adversarial-example-research/)è¯¦ç»†è®²è§£äº†åœ¨ç°å®ç”Ÿæ´»ä¸­ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆæš‚æ—¶æ— æ³•è§£é‡Šï¼‰çš„ä¸€äº›å±é™©æ€§ã€‚

> è¿™ä¸€é¢†åŸŸæœ‰å¾ˆå¤šçƒ­ç‚¹ç ”ç©¶ã€‚[è¿™äº›ä½œè€…](https://arxiv.org/abs/1611.03530)æœ€è¿‘æœç€æ­£ç¡®çš„æ–¹å‘è¿ˆå‡ºäº†ä¸€æ­¥ã€‚

# CNN åº”ç”¨æ¡ˆä¾‹

è¯¾å¤–èµ„æ–™
æ³¨ï¼šéƒ¨åˆ†èµ„æ–™æ¥è‡ªå›½å¤– youtube ä¸ google research.

**äº†è§£ [WaveNet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) æ¨¡å‹ã€‚**

> å¦‚æœä½ èƒ½è®­ç»ƒäººå·¥æ™ºèƒ½æœºå™¨äººå”±æ­Œï¼Œå¹²å˜›è¿˜è®­ç»ƒå®ƒèŠå¤©ï¼Ÿåœ¨ 2017 å¹´ 4 æœˆï¼Œç ”ç©¶äººå‘˜ä½¿ç”¨ WaveNet æ¨¡å‹çš„å˜ä½“ç”Ÿæˆäº†æ­Œæ›²ã€‚åŸå§‹è®ºæ–‡å’Œæ¼”ç¤ºå¯ä»¥åœ¨[æ­¤å¤„](http://www.creativeai.net/posts/W2C3baXvf2yJSLbY6/a-neural-parametric-singing-synthesizer)æ‰¾åˆ°ã€‚

**äº†è§£[æ–‡æœ¬åˆ†ç±»CNN](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/) ã€‚**

> ä½ æˆ–è®¸æƒ³æ³¨å†Œä½œè€…çš„æ·±åº¦å­¦ä¹ ç®€è®¯ï¼

**äº†è§£ Facebook çš„[åˆ›æ–° CNN æ–¹æ³•(Facebook)](https://code.facebook.com/posts/1978007565818999/a-novel-approach-to-neural-machine-translation/)**ï¼Œè¯¥æ–¹æ³•ä¸“é—¨ç”¨äºè§£å†³è¯­è¨€ç¿»è¯‘ä»»åŠ¡ï¼Œå‡†ç¡®ç‡è¾¾åˆ°äº†å‰æ²¿æ€§æ°´å¹³ï¼Œå¹¶ä¸”é€Ÿåº¦æ˜¯ RNN æ¨¡å‹çš„ 9 å€ã€‚

**åˆ©ç”¨ CNN å’Œå¼ºåŒ–å­¦ä¹ ç© [Atari](https://deepmind.com/research/dqn/) æ¸¸æˆã€‚ä½ å¯ä»¥[ä¸‹è½½](https://sites.google.com/a/deepmind.com/dqn/)æ­¤è®ºæ–‡é™„å¸¦çš„ä»£ç ã€‚**

> å¦‚æœä½ æƒ³ç ”ç©¶ä¸€äº›ï¼ˆæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼‰åˆå­¦è€…ä»£ç ï¼Œå»ºè®®ä½ å‚é˜… [Andrej Karpathy](http://karpathy.github.io/2016/05/31/rl/) çš„å¸–å­ã€‚

**åˆ©ç”¨ CNN [ç©çœ‹å›¾è¯´è¯æ¸¸æˆ](https://quickdraw.withgoogle.com/#)ï¼**

> æ­¤å¤–ï¼Œè¿˜å¯ä»¥å‚é˜… [A.I. Experiments](https://aiexperiments.withgoogle.com/) ç½‘ç«™ä¸Šçš„æ‰€æœ‰å…¶ä»–å¾ˆé…·çš„å®ç°ã€‚åˆ«å¿˜äº† [AutoDraw](https://www.autodraw.com/)ï¼

**è¯¦ç»†äº†è§£ [AlphaGo](https://deepmind.com/research/alphago/)ã€‚**

> é˜…è¯»[è¿™ç¯‡æ–‡ç« ](https://www.technologyreview.com/s/604273/finding-solace-in-defeat-by-artificial-intelligence/?set=604287)ï¼Œå…¶ä¸­æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šå¦‚æœæŒæ§ Goâ€œéœ€è¦äººç±»ç›´è§‰â€ï¼Œé‚£ä¹ˆäººæ€§å—åˆ°æŒ‘æˆ˜æ˜¯ä»€ä¹ˆæ„Ÿè§‰ï¼Ÿ_

**è§‚çœ‹è¿™äº›éå¸¸é…·çš„è§†é¢‘ï¼Œå…¶ä¸­çš„æ— äººæœºéƒ½å—åˆ° CNN çš„æ”¯æŒã€‚**

> è¿™æ˜¯åˆåˆ›ä¼ä¸š [Intelligent Flying Machines (IFM)](https://www.youtube.com/watch?v=AMDiR61f86Y) (Youtube)çš„è®¿è°ˆã€‚

> æˆ·å¤–è‡ªä¸»å¯¼èˆªé€šå¸¸éƒ½è¦å€ŸåŠ©[å…¨çƒå®šä½ç³»ç»Ÿ (GPS)](http://www.droneomega.com/gps-drone-navigation-works/)ï¼Œä½†æ˜¯ä¸‹é¢çš„æ¼”ç¤ºå±•ç¤ºçš„æ˜¯ç”± CNN æä¾›æŠ€æœ¯æ”¯æŒçš„[è‡ªä¸»æ— äººæœº](https://www.youtube.com/watch?v=wSFYOw4VIYY)(Youtube)ã€‚

**å¦‚æœä½ å¯¹æ— äººé©¾é©¶æ±½è½¦ä½¿ç”¨çš„ CNN æ„Ÿå…´è¶£ï¼Œè¯·å‚é˜…ï¼š**

> æˆ‘ä»¬çš„[æ— äººé©¾é©¶æ±½è½¦å·¥ç¨‹å¸ˆçº³ç±³å­¦ä½è¯¾ç¨‹](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013)ï¼Œæˆ‘ä»¬åœ¨[æ­¤é¡¹ç›®](https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project)ä¸­å¯¹[å¾·å›½äº¤é€šæ ‡å¿—](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset)æ•°æ®é›†ä¸­çš„æ ‡å¿—è¿›è¡Œåˆ†ç±»ã€‚

> è¿™äº›[ç³»åˆ—åšå®¢](https://pythonprogramming.net/game-frames-open-cv-python-plays-gta-v/)ï¼Œå…¶ä¸­è¯¦ç»†è®²è¿°äº†å¦‚ä½•è®­ç»ƒç”¨ Python ç¼–å†™çš„ CNNï¼Œä»¥ä¾¿ç”Ÿæˆèƒ½å¤Ÿç©â€œä¾ ç›—çŒè½¦æ‰‹â€çš„æ— äººé©¾é©¶ AIã€‚

**å‚é˜…è§†é¢‘ä¸­æ²¡æœ‰æåˆ°çš„å…¶ä»–åº”ç”¨æƒ…å½¢ã€‚**

> ä¸€äº›å…¨çƒæœ€è‘—åçš„ç”»ä½œè¢«[è½¬æ¢æˆäº†ä¸‰ç»´å½¢å¼](http://www.businessinsider.com/3d-printed-works-of-art-for-the-blind-2016-1)ï¼Œä»¥ä¾¿è§†åŠ›å—æŸäººå£«ä¹Ÿèƒ½æ¬£èµã€‚è™½ç„¶è¿™ç¯‡æ–‡ç« æ²¡æœ‰æåˆ°æ˜¯æ€ä¹ˆåšåˆ°çš„ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°å¯ä»¥ä½¿ç”¨ CNN [é¢„æµ‹å•ä¸ªå›¾ç‰‡çš„æ·±åº¦](https://www.cs.nyu.edu/~deigen/depth/)ã€‚

> å‚é˜…è¿™ç¯‡å…³äºä½¿ç”¨ CNN ç¡®å®šä¹³è…ºç™Œä½ç½®çš„[ç ”ç©¶è®ºæ–‡](https://research.googleblog.com/2017/03/assisting-pathologists-in-detecting.html)(google research)ã€‚

> CNN è¢«ç”¨æ¥[æ‹¯æ•‘æ¿’å±ç‰©ç§](https://blogs.nvidia.com/blog/2016/11/04/saving-endangered-species/?adbsc=social_20170303_70517416)ï¼

> ä¸€æ¬¾å«åš [FaceApp](http://www.digitaltrends.com/photography/faceapp-neural-net-image-editing/) çš„åº”ç”¨ä½¿ç”¨ CNN è®©ä½ åœ¨ç…§ç‰‡ä¸­æ˜¯å¾®ç¬‘çŠ¶æ€æˆ–æ”¹å˜æ€§åˆ«ã€‚

æ­¤å¤–ï¼Œæˆ‘æ³¨æ„åˆ°ä½ ä½¿ç”¨çš„å›¾ç‰‡æ˜¯ä»æˆ‘ä»¬æä¾›çš„æ•°æ®é›†ä¸­é€‰å–çš„ï¼Œæˆ‘éå¸¸ä¸æ¨èè¿™ç§åšæ³•ã€‚è¿™ç§è¡Œä¸ºå¯èƒ½ä¼šå¯¼è‡´æ ‡ç­¾æ³„éœ²ï¼ˆLabel Leakageï¼‰ï¼Œå¹¶ä¸èƒ½å¾ˆå¥½çš„è¯„ä¼°ä½ çš„æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å› ä¸ºï¼Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ¬èº«å°±æ˜¯åœ¨ä¸æ–­æ‹Ÿåˆè®­ç»ƒé›†ï¼Œå®ƒèƒ½å¾ˆå¥½åœ°é¢„æµ‹è®­ç»ƒé›†é‡Œçš„å›¾ç‰‡æ˜¯ç†æ‰€åº”å½“çš„ã€‚è€ŒéªŒè¯æ³›åŒ–èƒ½åŠ›æœ€å¥½çš„åšæ³•å°±æ˜¯ï¼Œä½¿ç”¨çœŸå®çš„ã€åœ¨è®­ç»ƒé›†/æµ‹è¯•é›†/éªŒè¯é›†éƒ½æ²¡æœ‰å‡ºç°è¿‡çš„å›¾ç‰‡æ¥è¿›è¡Œæµ‹è¯•ã€‚ä½ å¯ä»¥è‡ªç”±çš„ä½¿ç”¨ç½‘ä¸Šçš„å›¾ç‰‡æˆ–è€…è‡ªå·±çš„å›¾ç‰‡~ğŸ˜‰ åŒæ—¶ï¼Œå¸Œæœ›ä½ èƒ½å°è¯•ç±»å‹çš„å›¾ç‰‡æ¥è¿›è¡Œå®éªŒï¼Œæ¯”å¦‚çŒ«ã€å¤šæ¡ç‹—ï¼ˆå¯ä»¥æ˜¯ä¸åŒå“ç§ï¼‰ã€å¸¦ç€ç‹—è€³æœµçš„äººã€é£æ™¯ç…§ç­‰ã€‚æŒ‰ç…§æœºå™¨å­¦ä¹ çš„æ€è·¯ï¼Œä½ çš„è¾“å…¥è¦†ç›–çš„è¾“å…¥ç©ºé—´è¶Šå¤šï¼Œé‚£ä¹ˆä½ å°±èƒ½å¯¹æ¨¡å‹è¿›è¡Œè¶Šå¥½çš„è¯„ä¼°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä½ å°è¯•çš„å›¾ç‰‡ç±»å‹è¶Šå¤šï¼Œå¯¹æ¨¡å‹çš„è¯„ä¼°èƒ½åŠ›å°±è¶Šå¼ºã€‚ğŸ˜„

ä»¥ä¸‹æ˜¯æˆ‘å¯¹æ”¹è¿›æ¨¡å‹æå‡ºçš„å»ºè®®ï¼Œå¸Œæœ›å¯¹ä½ æœ‰å¸®åŠ©ï¼š

1.äº¤å‰éªŒè¯ï¼ˆCross Validationï¼‰ åœ¨æœ¬æ¬¡è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬åªè¿›è¡Œäº†ä¸€æ¬¡è®­ç»ƒé›†/æµ‹è¯•é›†åˆ‡åˆ†ï¼Œè€Œåœ¨å®é™…æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¾€å¾€æ˜¯ä½¿ç”¨äº¤å‰éªŒè¯ï¼ˆCross Validationï¼‰æ¥è¿›è¡Œæ¨¡å‹é€‰æ‹©ï¼ˆModel Selectionï¼‰å’Œè°ƒå‚ï¼ˆParameter Tunningï¼‰çš„ã€‚äº¤å‰éªŒè¯çš„é€šå¸¸åšæ³•æ˜¯ï¼ŒæŒ‰ç…§æŸç§æ–¹å¼å¤šæ¬¡è¿›è¡Œè®­ç»ƒé›†/æµ‹è¯•é›†åˆ‡åˆ†ï¼Œæœ€ç»ˆå–å¹³å‡å€¼ï¼ˆåŠ æƒå¹³å‡å€¼ï¼‰ï¼Œå…·ä½“å¯ä»¥å‚è€ƒç»´åŸºç™¾ç§‘)çš„ä»‹ç»ã€‚

2.æ¨¡å‹èåˆ/é›†æˆå­¦ä¹ ï¼ˆModel Ensemblingï¼‰ é€šè¿‡åˆ©ç”¨ä¸€äº›æœºå™¨å­¦ä¹ ä¸­æ¨¡å‹èåˆçš„æŠ€æœ¯ï¼Œå¦‚votingã€baggingã€blendingä»¥åŠstakingç­‰ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„å‡†ç¡®ç‡ä¸é²æ£’æ€§ï¼Œä¸”å‡ ä¹æ²¡æœ‰é£é™©ã€‚ä½ å¯ä»¥å‚è€ƒæˆ‘æ•´ç†çš„æœºå™¨å­¦ä¹ ç¬”è®°ä¸­çš„Ensembleéƒ¨åˆ†ã€‚

3.æ›´å¤šçš„æ•°æ® å¯¹äºæ·±åº¦å­¦ä¹ ï¼ˆæœºå™¨å­¦ä¹ ï¼‰ä»»åŠ¡æ¥è¯´ï¼Œæ›´å¤šçš„æ•°æ®æ„å‘³ç€æ›´ä¸ºä¸°å¯Œçš„è¾“å…¥ç©ºé—´ï¼Œå¯ä»¥å¸¦æ¥æ›´å¥½çš„è®­ç»ƒæ•ˆæœã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æ•°æ®å¢å¼ºï¼ˆData Augmentationï¼‰ã€å¯¹æŠ—ç”Ÿæˆç½‘ç»œï¼ˆGenerative Adversarial Networksï¼‰ç­‰æ–¹å¼æ¥å¯¹æ•°æ®é›†è¿›è¡Œæ‰©å……ï¼ŒåŒæ—¶è¿™ç§æ–¹å¼ä¹Ÿèƒ½æå‡æ¨¡å‹çš„é²æ£’æ€§ã€‚

4.æ›´æ¢äººè„¸æ£€æµ‹ç®—æ³• å°½ç®¡OpenCVå·¥å…·åŒ…éå¸¸æ–¹ä¾¿å¹¶ä¸”é«˜æ•ˆï¼ŒHaarçº§è”æ£€æµ‹ä¹Ÿæ˜¯ä¸€ä¸ªå¯ä»¥ç›´æ¥ä½¿ç”¨çš„å¼ºåŠ›ç®—æ³•ï¼Œä½†æ˜¯è¿™äº›ç®—æ³•ä»ç„¶ä¸èƒ½è·å¾—å¾ˆé«˜çš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸”éœ€è¦ç”¨æˆ·æä¾›æ­£é¢ç…§ç‰‡ï¼Œè¿™å¸¦æ¥çš„ä¸€å®šçš„ä¸ä¾¿ã€‚æ‰€ä»¥å¦‚æœæƒ³è¦è·å¾—æ›´å¥½çš„ç”¨æˆ·ä½“éªŒå’Œå‡†ç¡®ç‡ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä¸€äº›æ–°çš„äººè„¸è¯†åˆ«ç®—æ³•ï¼Œå¦‚åŸºäºæ·±åº¦å­¦ä¹ çš„ä¸€äº›ç®—æ³•ã€‚

5.å¤šç›®æ ‡ç›‘æµ‹ æ›´è¿›ä¸€æ­¥ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€äº›å…ˆè¿›çš„ç›®æ ‡è¯†åˆ«ç®—æ³•ï¼Œå¦‚RCNNã€Fast-RCNNã€Faster-RCNNæˆ–Masked-RCNNç­‰ï¼Œæ¥å®Œæˆä¸€å¼ ç…§ç‰‡ä¸­åŒæ—¶å‡ºç°å¤šä¸ªç›®æ ‡çš„æ£€æµ‹ä»»åŠ¡ã€‚

