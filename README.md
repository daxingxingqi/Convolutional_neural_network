 *convolutional neural network summary ÊòØ‰∏Ä‰∫õÂÖ≥‰∫éCNNÁöÑËµÑÊñôÊÄªÁªì„ÄÇ

 *CNN_example ÊòØÂü∫‰∫éÁêÜËß£ÊÄªÁªìÁöÑ‰∏Ä‰∫õCNNÁöÑÂ∫îÁî®ÂÆû‰æã

 *AI_doc ÊòØ‰∏Ä‰∏™Â∫îÁî®ÂÆû‰æã
 
 *AI_doc ÊòØ‰∏Ä‰∏™transfer learningÂ∫îÁî®ÂÆû‰æã
# Transfer Learning 
Êé®Ëçê‰Ω†ÈòÖËØª‰ª•‰∏ãÊùêÊñôÊù•Âä†Ê∑±ÂØπ CNNÂíåTransfer LearningÁöÑÁêÜËß£:

**[CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)**

ÁÇπËøôÈáåÊü•Áúã[Á¨îËÆ∞](https://github.com/daxingxingqi/CS231n-2017-Summary)

ÁÇπËøôÈáåÊü•ÁúãÂÆòÊñπ[Á¨îËÆ∞](https://cs231n.github.io/)

**[Using Convolutional Neural Networks to Classify Dog Breeds](http://cs231n.stanford.edu/reports/2015/pdfs/fcdh_FinalReport.pdf)**

**[Building an Image Classifier](https://towardsdatascience.com/learning-about-data-science-building-an-image-classifier-part-2-a7bcc6d5e825)**

**[Tips/Tricks in CNN](http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html)**

 - 1) data augmentation; 
 - 2) pre-processing on images; 
 - 3) initializations of Networks; 
 - 4) some tips during training; 
 - 5) selections of activation functions; 
 - 6) diverse regularizations; 
 - 7) some insights found from figures and finally 
 - 8) methods of ensemble multiple deep networks.

## [Transfer Learning using Keras](https://towardsdatascience.com/transfer-learning-using-keras-d804b2e04ef8)
1. **New dataset is small and similar to original dataset:**
There is a problem of over-fitting, if we try to train the entire network. Since the data is similar to the original data, we expect higher-level features in the ConvNet to be relevant to this dataset as well. Hence, the best idea might be to train a linear classifier on the CNN codes.

So lets freeze all the VGG19 layers and train only the classifier
```python
for layer in model.layers:
   layer.trainable = False
 
#Now we will be training only the classifiers (FC layers)
```
2. **New dataset is large and similar to the original dataset**
Since we have more data, we can have more confidence that we won‚Äôt overfit if we were to try to fine-tune through the full network.
``` python
for layer in model.layers:
   layer.trainable = True
#The default is already set to True. I have mentioned it here to make things clear.
```
In case if you want to freeze the first few layers as these layers will be detecting edges and blobs, you can freeze them by using the following code.
```python
for layer in model.layers[:5]:
   layer.trainable = False.
# Here I am freezing the first 5 layers 
```
3. **New dataset is small but very different from the original dataset**
Since the dataset is very small, We may want to extract the features from the earlier layer and train a classifier on top of that. This requires a little bit of knowledge on h5py.

The above code should help. It will extract the ‚Äúblock2_pool‚Äù features. In general this is not helpful as this layer has (64*64*128) features and training a classifier on top of it might not help us exactly. We can add a few FC layers and train a neural network on top of it. That should be straight forward.

*Add few FC layers and output layer.

*Set the weights for earlier layers and freeze them.

*Train the network.

4. **New dataset is large and very different from the original dataset.**
This is straight forward. since you have large dataset, you can design your own network or use the existing ones.

*Train the network using random initialisations or use the pre-trained network weights as initialisers. The second one is generally preferred.

*If you are using a different network or making small modification here and there for the existing network, Be careful with the naming conventions.

>[Transfer Learning in TensorFlow on the Kaggle Rainforest competition](https://medium.com/@luckylwk/transfer-learning-in-tensorflow-on-the-kaggle-rainforest-competition-4e978fadb571)

>[Transfer Learning and Fine-tuning](https://medium.com/deeplearningsandbox/how-to-use-transfer-learning-and-fine-tuning-in-keras-and-tensorflow-to-build-an-image-recognition-94b0b02444f2)

Áõ∏ÂÖ≥ËÆ∫Êñá:

**[[VGG16] VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION](https://arxiv.org/abs/1409.1556)**
- VGG16
``` python
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout, Flatten
from keras.layers import Conv2D
from keras.layers import MaxPooling2D

input_shape = (224, 224, 3)

model = Sequential([
    Conv2D(64, (3, 3), input_shape=input_shape, padding='same',
           activation='relu'),
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(128, (3, 3), activation='relu', padding='same'),
    Conv2D(128, (3, 3), activation='relu', padding='same',),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Flatten(),
    Dense(4096, activation='relu'),
    Dense(4096, activation='relu'),
    Dense(1000, activation='softmax')
])

model.summary()
```
- VGG19
``` python
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout, Flatten
from keras.layers import Conv2D
from keras.layers import MaxPooling2D

input_shape = (224, 224, 3)

model = Sequential([
    Conv2D(64, (3, 3), input_shape=input_shape, padding='same',
           activation='relu'),
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(128, (3, 3), activation='relu', padding='same'),
    Conv2D(128, (3, 3), activation='relu', padding='same',),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    Conv2D(256, (3, 3), activation='relu', padding='same',)
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',)
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',)
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Flatten(),
    Dense(4096, activation='relu'),
    Dense(4096, activation='relu'),
    Dense(1000, activation='softmax')
])

model.summary()
```

**[[Inception-v1] Going deeper with convolutions](https://arxiv.org/abs/1409.4842)**

ËÆ≤Ëß£-https://becominghuman.ai/understanding-and-coding-inception-module-in-keras-eb56e9056b4b
<div align=center><img width="550" src=resource/1.png></div>

``` python
#-*- coding: UTF-8 -*-
"""
Author: lanbing510
Environment: Keras2.0.5ÔºåPython2.7
Model: GoogLeNet Inception V1
"""

from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D
from keras.layers import Flatten, Dense, Dropout
from keras.layers import Input, concatenate
from keras.models import Model
from keras import regularizers
from keras.utils import plot_model
from KerasLayers.Custom_layers import LRN2D


# Global Constants
NB_CLASS=1000
LEARNING_RATE=0.01
MOMENTUM=0.9
ALPHA=0.0001
BETA=0.75
GAMMA=0.1
DROPOUT=0.4
WEIGHT_DECAY=0.0005
LRN2D_NORM=True
DATA_FORMAT='channels_last' # Theano:'channels_first' Tensorflow:'channels_last'
USE_BN=True


def conv2D_lrn2d(x,filters,kernel_size,strides=(1,1),padding='same',data_format=DATA_FORMAT,dilation_rate=(1,1),activation='relu',use_bias=True,kernel_initializer='glorot_uniform',bias_initializer='zeros',kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,lrn2d_norm=LRN2D_NORM,weight_decay=WEIGHT_DECAY):
    if weight_decay:
        kernel_regularizer=regularizers.l2(weight_decay)
        bias_regularizer=regularizers.l2(weight_decay)
    else:
        kernel_regularizer=None
        bias_regularizer=None

    x=Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)

    if lrn2d_norm:
        x=LRN2D(alpha=ALPHA,beta=BETA)(x)

    return x



def inception_module(x,params,concat_axis,padding='same',data_format=DATA_FORMAT,dilation_rate=(1,1),activation='relu',use_bias=True,kernel_initializer='glorot_uniform',bias_initializer='zeros',kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,lrn2d_norm=LRN2D_NORM,weight_decay=None):
    (branch1,branch2,branch3,branch4)=params
    if weight_decay:
        kernel_regularizer=regularizers.l2(weight_decay)
        bias_regularizer=regularizers.l2(weight_decay)
    else:
        kernel_regularizer=None
        bias_regularizer=None

    pathway1=Conv2D(filters=branch1[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)

    pathway2=Conv2D(filters=branch2[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)
    pathway2=Conv2D(filters=branch2[1],kernel_size=(3,3),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway2)

    pathway3=Conv2D(filters=branch3[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)
    pathway3=Conv2D(filters=branch3[1],kernel_size=(5,5),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway3)

    pathway4=MaxPooling2D(pool_size=(3,3),strides=1,padding=padding,data_format=DATA_FORMAT)(x)
    pathway4=Conv2D(filters=branch4[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway4)

    return concatenate([pathway1,pathway2,pathway3,pathway4],axis=concat_axis)



def create_model():
    if DATA_FORMAT=='channels_first':
        INP_SHAPE=(3,224,224)
        img_input=Input(shape=INP_SHAPE)
        CONCAT_AXIS=1
    elif DATA_FORMAT=='channels_last':
        INP_SHAPE=(224,224,3)
        img_input=Input(shape=INP_SHAPE)
        CONCAT_AXIS=3
    else:
        raise Exception('Invalid Dim Ordering: '+str(DIM_ORDERING))

    x=conv2D_lrn2d(img_input,64,(7,7),2,padding='same',lrn2d_norm=False)
    x=MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x)
    x=LRN2D(alpha=ALPHA,beta=BETA)(x)

    x=conv2D_lrn2d(x,64,(1,1),1,padding='same',lrn2d_norm=False)

    x=conv2D_lrn2d(x,192,(3,3),1,padding='same',lrn2d_norm=True)
    x=MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x)

    x=inception_module(x,params=[(64,),(96,128),(16,32),(32,)],concat_axis=CONCAT_AXIS) #3a
    x=inception_module(x,params=[(128,),(128,192),(32,96),(64,)],concat_axis=CONCAT_AXIS) #3b
    x=MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x)

    x=inception_module(x,params=[(192,),(96,208),(16,48),(64,)],concat_axis=CONCAT_AXIS) #4a
    x=inception_module(x,params=[(160,),(112,224),(24,64),(64,)],concat_axis=CONCAT_AXIS) #4b
    x=inception_module(x,params=[(128,),(128,256),(24,64),(64,)],concat_axis=CONCAT_AXIS) #4c
    x=inception_module(x,params=[(112,),(144,288),(32,64),(64,)],concat_axis=CONCAT_AXIS) #4d
    x=inception_module(x,params=[(256,),(160,320),(32,128),(128,)],concat_axis=CONCAT_AXIS) #4e
    x=MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x)

    x=inception_module(x,params=[(256,),(160,320),(32,128),(128,)],concat_axis=CONCAT_AXIS) #5a
    x=inception_module(x,params=[(384,),(192,384),(48,128),(128,)],concat_axis=CONCAT_AXIS) #5b
    x=AveragePooling2D(pool_size=(7,7),strides=1,padding='valid',data_format=DATA_FORMAT)(x)

    x=Flatten()(x)
    x=Dropout(DROPOUT)(x)
    x=Dense(output_dim=NB_CLASS,activation='linear')(x)
    x=Dense(output_dim=NB_CLASS,activation='softmax')(x)

    return x,img_input,CONCAT_AXIS,INP_SHAPE,DATA_FORMAT


def check_print():
    # Create the Model
    x,img_input,CONCAT_AXIS,INP_SHAPE,DATA_FORMAT=create_model()

    # Create a Keras Model
    model=Model(input=img_input,output=[x])
    model.summary()

    # Save a PNG of the Model Build
    plot_model(model,to_file='GoogLeNet.png')

    model.compile(optimizer='rmsprop',loss='categorical_crossentropy')
    print 'Model Compiled'


if __name__=='__main__':
    check_print() 
```
 **[[Inception V2], Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://arxiv.org/abs/1502.03167)**

Inception V2ÁöÑÁΩëÁªúÂú®Inception v1ÁöÑÂü∫Á°Ä‰∏äÔºåËøõË°å‰∫ÜÊîπËøõÔºå‰∏ÄÊñπÈù¢‰∫ÜÂä†ÂÖ•‰∫ÜBNÂ±ÇÔºåÂáèÂ∞ë‰∫ÜInternal Covariate ShiftÔºàÂÜÖÈÉ®Á•ûÁªèÂÖÉÂàÜÂ∏ÉÁöÑÊîπÂèòÔºâÔºå‰ΩøÊØè‰∏ÄÂ±ÇÁöÑËæìÂá∫ÈÉΩËßÑËåÉÂåñÂà∞‰∏Ä‰∏™N(0, 1)ÁöÑÈ´òÊñØÔºåËøòÂéªÈô§‰∫ÜDropout„ÄÅLRNÁ≠âÁªìÊûÑÔºõÂè¶Â§ñ‰∏ÄÊñπÈù¢Â≠¶‰π†VGGÁî®2‰∏™3x3ÁöÑÂç∑ÁßØÊõø‰ª£inceptionÊ®°Âùó‰∏≠ÁöÑ5x5Âç∑ÁßØÔºåÊó¢Èôç‰Ωé‰∫ÜÂèÇÊï∞Êï∞ÈáèÔºåÂèàÂä†ÈÄüËÆ°ÁÆó„ÄÇ

**[[Inception-v3] Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567)**
 
 Inception V3‰∏Ä‰∏™ÊúÄÈáçË¶ÅÁöÑÊîπËøõÊòØÂàÜËß£ÔºàFactorizationÔºâÔºåÂ∞Ü7x7ÂàÜËß£Êàê‰∏§‰∏™‰∏ÄÁª¥ÁöÑÂç∑ÁßØÔºà1x7,7x1ÔºâÔºå3x3‰πüÊòØ‰∏ÄÊ†∑Ôºà1x3,3x1Ôºâ„ÄÇËøôÊ†∑ÁöÑÂ•ΩÂ§ÑÔºåÊó¢ÂèØ‰ª•Âä†ÈÄüËÆ°ÁÆóÔºàÂ§ö‰ΩôÁöÑËÆ°ÁÆóËÉΩÂäõÂèØ‰ª•Áî®Êù•Âä†Ê∑±ÁΩëÁªúÔºâÔºåÂèàÂèØ‰ª•Â∞Ü1‰∏™convÊãÜÊàê2‰∏™convÔºå‰ΩøÂæóÁΩëÁªúÊ∑±Â∫¶Ëøõ‰∏ÄÊ≠•Â¢ûÂä†ÔºåÂ¢ûÂä†‰∫ÜÁΩëÁªúÁöÑÈùûÁ∫øÊÄßÔºåÂèØ‰ª•Â§ÑÁêÜÊõ¥Â§öÊõ¥‰∏∞ÂØåÁöÑÁ©∫Èó¥ÁâπÂæÅÔºåÂ¢ûÂä†ÁâπÂæÅÂ§öÊ†∑ÊÄß„ÄÇËøòÊúâÂÄºÂæóÊ≥®ÊÑèÁöÑÂú∞ÊñπÊòØÁΩëÁªúËæìÂÖ•‰ªé224x224Âèò‰∏∫‰∫Ü299x299ÔºåÊõ¥Âä†Á≤æÁªÜËÆæËÆ°‰∫Ü35x35/17x17/8x8ÁöÑÊ®°Âùó„ÄÇ
 
**[[Inception-v4] Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/abs/1602.07261)**

Inception V4ÁªìÂêà‰∫ÜÂæÆËΩØÁöÑResNetÔºåÂèëÁé∞ResNetÁöÑÁªìÊûÑÂèØ‰ª•ÊûÅÂ§ßÂú∞Âä†ÈÄüËÆ≠ÁªÉÔºåÂêåÊó∂ÊÄßËÉΩ‰πüÊúâÊèêÂçáÔºåÂæóÂà∞‰∏Ä‰∏™Inception-ResNet V2ÁΩëÁªúÔºåÂêåÊó∂ËøòËÆæËÆ°‰∫Ü‰∏Ä‰∏™Êõ¥Ê∑±Êõ¥‰ºòÂåñÁöÑInception V4Ê®°ÂûãÔºåËÉΩËææÂà∞‰∏éInception-ResNet V2Áõ∏Â™≤ÁæéÁöÑÊÄßËÉΩ„ÄÇ

**[[ResNet] Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)**

ËÆ≤Ëß£-https://blog.waya.ai/deep-residual-learning-9610bb62c355

**[[Xception] Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357)**

ËÆ≤Ëß£-https://blog.csdn.net/u014380165/article/details/75142710

# CNN Â∫îÁî®Ê°à‰æã

ËØæÂ§ñËµÑÊñô
Ê≥®ÔºöÈÉ®ÂàÜËµÑÊñôÊù•Ëá™ÂõΩÂ§ñ youtube ‰∏é google research.

**‰∫ÜËß£ [WaveNet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) Ê®°Âûã„ÄÇ**

>Â¶ÇÊûú‰Ω†ËÉΩËÆ≠ÁªÉ‰∫∫Â∑•Êô∫ËÉΩÊú∫Âô®‰∫∫Âî±Ê≠åÔºåÂπ≤ÂòõËøòËÆ≠ÁªÉÂÆÉËÅäÂ§©ÔºüÂú® 2017 Âπ¥ 4 ÊúàÔºåÁ†îÁ©∂‰∫∫Âëò‰ΩøÁî® WaveNet Ê®°ÂûãÁöÑÂèò‰ΩìÁîüÊàê‰∫ÜÊ≠åÊõ≤„ÄÇÂéüÂßãËÆ∫ÊñáÂíåÊºîÁ§∫ÂèØ‰ª•Âú®[Ê≠§Â§Ñ](http://www.creativeai.net/posts/W2C3baXvf2yJSLbY6/a-neural-parametric-singing-synthesizer)ÊâæÂà∞„ÄÇ

**‰∫ÜËß£[ÊñáÊú¨ÂàÜÁ±ªCNN](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/) „ÄÇ**

>‰Ω†ÊàñËÆ∏ÊÉ≥Ê≥®ÂÜå‰ΩúËÄÖÁöÑÊ∑±Â∫¶Â≠¶‰π†ÁÆÄËÆØÔºÅ

**‰∫ÜËß£ Facebook ÁöÑ[ÂàõÊñ∞ CNN ÊñπÊ≥ï(Facebook)](https://code.facebook.com/posts/1978007565818999/a-novel-approach-to-neural-machine-translation/)**ÔºåËØ•ÊñπÊ≥ï‰∏ìÈó®Áî®‰∫éËß£ÂÜ≥ËØ≠Ë®ÄÁøªËØë‰ªªÂä°ÔºåÂáÜÁ°ÆÁéáËææÂà∞‰∫ÜÂâçÊ≤øÊÄßÊ∞¥Âπ≥ÔºåÂπ∂‰∏îÈÄüÂ∫¶ÊòØ RNN Ê®°ÂûãÁöÑ 9 ÂÄç„ÄÇ

**Âà©Áî® CNN ÂíåÂº∫ÂåñÂ≠¶‰π†Áé© [Atari](https://deepmind.com/research/dqn/) Ê∏∏Êàè„ÄÇ‰Ω†ÂèØ‰ª•[‰∏ãËΩΩ](https://sites.google.com/a/deepmind.com/dqn/)Ê≠§ËÆ∫ÊñáÈôÑÂ∏¶ÁöÑ‰ª£Á†Å„ÄÇ**

>Â¶ÇÊûú‰Ω†ÊÉ≥Á†îÁ©∂‰∏Ä‰∫õÔºàÊ∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†ÔºâÂàùÂ≠¶ËÄÖ‰ª£Á†ÅÔºåÂª∫ËÆÆ‰Ω†ÂèÇÈòÖ [Andrej Karpathy](http://karpathy.github.io/2016/05/31/rl/) ÁöÑÂ∏ñÂ≠ê„ÄÇ

**Âà©Áî® CNN [Áé©ÁúãÂõæËØ¥ËØçÊ∏∏Êàè](https://quickdraw.withgoogle.com/#)ÔºÅ**

>Ê≠§Â§ñÔºåËøòÂèØ‰ª•ÂèÇÈòÖ [A.I. Experiments](https://aiexperiments.withgoogle.com/) ÁΩëÁ´ô‰∏äÁöÑÊâÄÊúâÂÖ∂‰ªñÂæàÈÖ∑ÁöÑÂÆûÁé∞„ÄÇÂà´Âøò‰∫Ü [AutoDraw](https://www.autodraw.com/)ÔºÅ

**ËØ¶ÁªÜ‰∫ÜËß£ [AlphaGo](https://deepmind.com/research/alphago/)„ÄÇ**

>ÈòÖËØª[ËøôÁØáÊñáÁ´†](https://www.technologyreview.com/s/604273/finding-solace-in-defeat-by-artificial-intelligence/?set=604287)ÔºåÂÖ∂‰∏≠ÊèêÂá∫‰∫Ü‰∏Ä‰∏™ÈóÆÈ¢òÔºöÂ¶ÇÊûúÊéåÊéß Go‚ÄúÈúÄË¶Å‰∫∫Á±ªÁõ¥Ëßâ‚ÄùÔºåÈÇ£‰πà‰∫∫ÊÄßÂèóÂà∞ÊåëÊàòÊòØ‰ªÄ‰πàÊÑüËßâÔºü_

**ËßÇÁúãËøô‰∫õÈùûÂ∏∏ÈÖ∑ÁöÑËßÜÈ¢ëÔºåÂÖ∂‰∏≠ÁöÑÊó†‰∫∫Êú∫ÈÉΩÂèóÂà∞ CNN ÁöÑÊîØÊåÅ„ÄÇ**

>ËøôÊòØÂàùÂàõ‰ºÅ‰∏ö [Intelligent Flying Machines (IFM)](https://www.youtube.com/watch?v=AMDiR61f86Y) (Youtube)ÁöÑËÆøË∞à„ÄÇ

>Êà∑Â§ñËá™‰∏ªÂØºËà™ÈÄöÂ∏∏ÈÉΩË¶ÅÂÄüÂä©[ÂÖ®ÁêÉÂÆö‰ΩçÁ≥ªÁªü (GPS)](http://www.droneomega.com/gps-drone-navigation-works/)Ôºå‰ΩÜÊòØ‰∏ãÈù¢ÁöÑÊºîÁ§∫Â±ïÁ§∫ÁöÑÊòØÁî± CNN Êèê‰æõÊäÄÊúØÊîØÊåÅÁöÑ[Ëá™‰∏ªÊó†‰∫∫Êú∫](https://www.youtube.com/watch?v=wSFYOw4VIYY)(Youtube)„ÄÇ

**Â¶ÇÊûú‰Ω†ÂØπÊó†‰∫∫È©æÈ©∂Ê±ΩËΩ¶‰ΩøÁî®ÁöÑ CNN ÊÑüÂÖ¥Ë∂£ÔºåËØ∑ÂèÇÈòÖÔºö**

>Êàë‰ª¨ÁöÑ[Êó†‰∫∫È©æÈ©∂Ê±ΩËΩ¶Â∑•Á®ãÂ∏àÁ∫≥Á±≥Â≠¶‰ΩçËØæÁ®ã](https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013)ÔºåÊàë‰ª¨Âú®[Ê≠§È°πÁõÆ](https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project)‰∏≠ÂØπ[Âæ∑ÂõΩ‰∫§ÈÄöÊ†áÂøó](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset)Êï∞ÊçÆÈõÜ‰∏≠ÁöÑÊ†áÂøóËøõË°åÂàÜÁ±ª„ÄÇ

>Ëøô‰∫õ[Á≥ªÂàóÂçöÂÆ¢](https://pythonprogramming.net/game-frames-open-cv-python-plays-gta-v/)ÔºåÂÖ∂‰∏≠ËØ¶ÁªÜËÆ≤Ëø∞‰∫ÜÂ¶Ç‰ΩïËÆ≠ÁªÉÁî® Python ÁºñÂÜôÁöÑ CNNÔºå‰ª•‰æøÁîüÊàêËÉΩÂ§üÁé©‚Äú‰æ†ÁõóÁåéËΩ¶Êâã‚ÄùÁöÑÊó†‰∫∫È©æÈ©∂ AI„ÄÇ

**ÂèÇÈòÖËßÜÈ¢ë‰∏≠Ê≤°ÊúâÊèêÂà∞ÁöÑÂÖ∂‰ªñÂ∫îÁî®ÊÉÖÂΩ¢„ÄÇ**

>‰∏Ä‰∫õÂÖ®ÁêÉÊúÄËëóÂêçÁöÑÁîª‰ΩúË¢´[ËΩ¨Êç¢Êàê‰∫Ü‰∏âÁª¥ÂΩ¢Âºè](http://www.businessinsider.com/3d-printed-works-of-art-for-the-blind-2016-1)Ôºå‰ª•‰æøËßÜÂäõÂèóÊçü‰∫∫Â£´‰πüËÉΩÊ¨£Ëµè„ÄÇËôΩÁÑ∂ËøôÁØáÊñáÁ´†Ê≤°ÊúâÊèêÂà∞ÊòØÊÄé‰πàÂÅöÂà∞ÁöÑÔºåÊàë‰ª¨Ê≥®ÊÑèÂà∞ÂèØ‰ª•‰ΩøÁî® CNN [È¢ÑÊµãÂçï‰∏™ÂõæÁâáÁöÑÊ∑±Â∫¶](https://www.cs.nyu.edu/~deigen/depth/)„ÄÇ

>ÂèÇÈòÖËøôÁØáÂÖ≥‰∫é‰ΩøÁî® CNN Á°ÆÂÆö‰π≥ËÖ∫Áôå‰ΩçÁΩÆÁöÑ[Á†îÁ©∂ËÆ∫Êñá](https://research.googleblog.com/2017/03/assisting-pathologists-in-detecting.html)(google research)„ÄÇ

>CNN Ë¢´Áî®Êù•[ÊãØÊïëÊøíÂç±Áâ©Áßç](https://blogs.nvidia.com/blog/2016/11/04/saving-endangered-species/?adbsc=social_20170303_70517416)ÔºÅ

>‰∏ÄÊ¨æÂè´ÂÅö [FaceApp](http://www.digitaltrends.com/photography/faceapp-neural-net-image-editing/) ÁöÑÂ∫îÁî®‰ΩøÁî® CNN ËÆ©‰Ω†Âú®ÁÖßÁâá‰∏≠ÊòØÂæÆÁ¨ëÁä∂ÊÄÅÊàñÊîπÂèòÊÄßÂà´„ÄÇ


Ê≠§Â§ñÔºåÊàëÊ≥®ÊÑèÂà∞‰Ω†‰ΩøÁî®ÁöÑÂõæÁâáÊòØ‰ªéÊàë‰ª¨Êèê‰æõÁöÑÊï∞ÊçÆÈõÜ‰∏≠ÈÄâÂèñÁöÑÔºåÊàëÈùûÂ∏∏‰∏çÊé®ËçêËøôÁßçÂÅöÊ≥ï„ÄÇËøôÁßçË°å‰∏∫ÂèØËÉΩ‰ºöÂØºËá¥Ê†áÁ≠æÊ≥ÑÈú≤ÔºàLabel LeakageÔºâÔºåÂπ∂‰∏çËÉΩÂæàÂ•ΩÁöÑËØÑ‰º∞‰Ω†ÁöÑÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇÂõ†‰∏∫ÔºåÊ®°ÂûãÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Êú¨Ë∫´Â∞±ÊòØÂú®‰∏çÊñ≠ÊãüÂêàËÆ≠ÁªÉÈõÜÔºåÂÆÉËÉΩÂæàÂ•ΩÂú∞È¢ÑÊµãËÆ≠ÁªÉÈõÜÈáåÁöÑÂõæÁâáÊòØÁêÜÊâÄÂ∫îÂΩìÁöÑ„ÄÇËÄåÈ™åËØÅÊ≥õÂåñËÉΩÂäõÊúÄÂ•ΩÁöÑÂÅöÊ≥ïÂ∞±ÊòØÔºå‰ΩøÁî®ÁúüÂÆûÁöÑ„ÄÅÂú®ËÆ≠ÁªÉÈõÜ/ÊµãËØïÈõÜ/È™åËØÅÈõÜÈÉΩÊ≤°ÊúâÂá∫Áé∞ËøáÁöÑÂõæÁâáÊù•ËøõË°åÊµãËØï„ÄÇ‰Ω†ÂèØ‰ª•Ëá™Áî±ÁöÑ‰ΩøÁî®ÁΩë‰∏äÁöÑÂõæÁâáÊàñËÄÖËá™Â∑±ÁöÑÂõæÁâá~üòâ ÂêåÊó∂ÔºåÂ∏åÊúõ‰Ω†ËÉΩÂ∞ùËØïÁ±ªÂûãÁöÑÂõæÁâáÊù•ËøõË°åÂÆûÈ™åÔºåÊØîÂ¶ÇÁå´„ÄÅÂ§öÊù°ÁãóÔºàÂèØ‰ª•ÊòØ‰∏çÂêåÂìÅÁßçÔºâ„ÄÅÂ∏¶ÁùÄÁãóËÄ≥ÊúµÁöÑ‰∫∫„ÄÅÈ£éÊôØÁÖßÁ≠â„ÄÇÊåâÁÖßÊú∫Âô®Â≠¶‰π†ÁöÑÊÄùË∑ØÔºå‰Ω†ÁöÑËæìÂÖ•Ë¶ÜÁõñÁöÑËæìÂÖ•Á©∫Èó¥Ë∂äÂ§öÔºåÈÇ£‰πà‰Ω†Â∞±ËÉΩÂØπÊ®°ÂûãËøõË°åË∂äÂ•ΩÁöÑËØÑ‰º∞„ÄÇ‰πüÂ∞±ÊòØËØ¥Ôºå‰Ω†Â∞ùËØïÁöÑÂõæÁâáÁ±ªÂûãË∂äÂ§öÔºåÂØπÊ®°ÂûãÁöÑËØÑ‰º∞ËÉΩÂäõÂ∞±Ë∂äÂº∫„ÄÇüòÑ

‰ª•‰∏ãÊòØÊàëÂØπÊîπËøõÊ®°ÂûãÊèêÂá∫ÁöÑÂª∫ËÆÆÔºåÂ∏åÊúõÂØπ‰Ω†ÊúâÂ∏ÆÂä©Ôºö

1.‰∫§ÂèâÈ™åËØÅÔºàCross ValidationÔºâ Âú®Êú¨Ê¨°ËÆ≠ÁªÉ‰∏≠ÔºåÊàë‰ª¨Âè™ËøõË°å‰∫Ü‰∏ÄÊ¨°ËÆ≠ÁªÉÈõÜ/ÊµãËØïÈõÜÂàáÂàÜÔºåËÄåÂú®ÂÆûÈôÖÊ®°ÂûãËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊàë‰ª¨ÂæÄÂæÄÊòØ‰ΩøÁî®‰∫§ÂèâÈ™åËØÅÔºàCross ValidationÔºâÊù•ËøõË°åÊ®°ÂûãÈÄâÊã©ÔºàModel SelectionÔºâÂíåË∞ÉÂèÇÔºàParameter TunningÔºâÁöÑ„ÄÇ‰∫§ÂèâÈ™åËØÅÁöÑÈÄöÂ∏∏ÂÅöÊ≥ïÊòØÔºåÊåâÁÖßÊüêÁßçÊñπÂºèÂ§öÊ¨°ËøõË°åËÆ≠ÁªÉÈõÜ/ÊµãËØïÈõÜÂàáÂàÜÔºåÊúÄÁªàÂèñÂπ≥ÂùáÂÄºÔºàÂä†ÊùÉÂπ≥ÂùáÂÄºÔºâÔºåÂÖ∑‰ΩìÂèØ‰ª•ÂèÇËÄÉÁª¥Âü∫ÁôæÁßë)ÁöÑ‰ªãÁªç„ÄÇ

2.Ê®°ÂûãËûçÂêà/ÈõÜÊàêÂ≠¶‰π†ÔºàModel EnsemblingÔºâ ÈÄöËøáÂà©Áî®‰∏Ä‰∫õÊú∫Âô®Â≠¶‰π†‰∏≠Ê®°ÂûãËûçÂêàÁöÑÊäÄÊúØÔºåÂ¶Çvoting„ÄÅbagging„ÄÅblending‰ª•ÂèästakingÁ≠âÔºåÂèØ‰ª•ÊòæËëóÊèêÈ´òÊ®°ÂûãÁöÑÂáÜÁ°ÆÁéá‰∏éÈ≤ÅÊ£íÊÄßÔºå‰∏îÂá†‰πéÊ≤°ÊúâÈ£éÈô©„ÄÇ‰Ω†ÂèØ‰ª•ÂèÇËÄÉÊàëÊï¥ÁêÜÁöÑÊú∫Âô®Â≠¶‰π†Á¨îËÆ∞‰∏≠ÁöÑEnsembleÈÉ®ÂàÜ„ÄÇ

3.Êõ¥Â§öÁöÑÊï∞ÊçÆ ÂØπ‰∫éÊ∑±Â∫¶Â≠¶‰π†ÔºàÊú∫Âô®Â≠¶‰π†Ôºâ‰ªªÂä°Êù•ËØ¥ÔºåÊõ¥Â§öÁöÑÊï∞ÊçÆÊÑèÂë≥ÁùÄÊõ¥‰∏∫‰∏∞ÂØåÁöÑËæìÂÖ•Á©∫Èó¥ÔºåÂèØ‰ª•Â∏¶Êù•Êõ¥Â•ΩÁöÑËÆ≠ÁªÉÊïàÊûú„ÄÇÊàë‰ª¨ÂèØ‰ª•ÈÄöËøáÊï∞ÊçÆÂ¢ûÂº∫ÔºàData AugmentationÔºâ„ÄÅÂØπÊäóÁîüÊàêÁΩëÁªúÔºàGenerative Adversarial NetworksÔºâÁ≠âÊñπÂºèÊù•ÂØπÊï∞ÊçÆÈõÜËøõË°åÊâ©ÂÖÖÔºåÂêåÊó∂ËøôÁßçÊñπÂºè‰πüËÉΩÊèêÂçáÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ

4.Êõ¥Êç¢‰∫∫ËÑ∏Ê£ÄÊµãÁÆóÊ≥ï Â∞ΩÁÆ°OpenCVÂ∑•ÂÖ∑ÂåÖÈùûÂ∏∏Êñπ‰æøÂπ∂‰∏îÈ´òÊïàÔºåHaarÁ∫ßËÅîÊ£ÄÊµã‰πüÊòØ‰∏Ä‰∏™ÂèØ‰ª•Áõ¥Êé•‰ΩøÁî®ÁöÑÂº∫ÂäõÁÆóÊ≥ïÔºå‰ΩÜÊòØËøô‰∫õÁÆóÊ≥ï‰ªçÁÑ∂‰∏çËÉΩËé∑ÂæóÂæàÈ´òÁöÑÂáÜÁ°ÆÁéáÔºåÂπ∂‰∏îÈúÄË¶ÅÁî®Êà∑Êèê‰æõÊ≠£Èù¢ÁÖßÁâáÔºåËøôÂ∏¶Êù•ÁöÑ‰∏ÄÂÆöÁöÑ‰∏ç‰æø„ÄÇÊâÄ‰ª•Â¶ÇÊûúÊÉ≥Ë¶ÅËé∑ÂæóÊõ¥Â•ΩÁöÑÁî®Êà∑‰ΩìÈ™åÂíåÂáÜÁ°ÆÁéáÔºåÊàë‰ª¨ÂèØ‰ª•Â∞ùËØï‰∏Ä‰∫õÊñ∞ÁöÑ‰∫∫ËÑ∏ËØÜÂà´ÁÆóÊ≥ïÔºåÂ¶ÇÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑ‰∏Ä‰∫õÁÆóÊ≥ï„ÄÇ

5.Â§öÁõÆÊ†áÁõëÊµã Êõ¥Ëøõ‰∏ÄÊ≠•ÔºåÊàë‰ª¨ÂèØ‰ª•ÈÄöËøá‰∏Ä‰∫õÂÖàËøõÁöÑÁõÆÊ†áËØÜÂà´ÁÆóÊ≥ïÔºåÂ¶ÇRCNN„ÄÅFast-RCNN„ÄÅFaster-RCNNÊàñMasked-RCNNÁ≠âÔºåÊù•ÂÆåÊàê‰∏ÄÂº†ÁÖßÁâá‰∏≠ÂêåÊó∂Âá∫Áé∞Â§ö‰∏™ÁõÆÊ†áÁöÑÊ£ÄÊµã‰ªªÂä°„ÄÇ

# ÊçüÂ§±Âíå‰ºòÂåñÁÆóÊ≥ï

*ÊçüÂ§±ÂáΩÊï∞ÊòØÁî®Êù•‰º∞ÈáèÊ®°Âûã‰∏≠È¢ÑÊµãÂÄºy‰∏éÁúüÂÆûÂÄºY‰πãÈó¥ÁöÑÂ∑ÆÂºÇÔºåÂç≥‰∏ç‰∏ÄËá¥Á®ãÂ∫¶

Â¶ÇÊûú‰Ω†ÊÉ≥ËØ¶ÁªÜ‰∫ÜËß£ Keras ‰∏≠ÁöÑÂÆåÂÖ®ËøûÊé•Â±ÇÔºåËØ∑ÈòÖËØªËøôÁØáÂÖ≥‰∫éÂØÜÈõÜÂ±ÇÁöÑ[ÊñáÊ°£](https://keras.io/layers/core/)„ÄÇ‰Ω†ÂèØ‰ª•ÈÄöËøá‰∏∫ **kernel_initializer** Âíå **bias_initializer** ÂèÇÊï∞Êèê‰æõÂÄºÊõ¥ÊîπÊùÉÈáçÁöÑÂàùÂßãÂåñÊñπÊ≥ï„ÄÇÊ≥®ÊÑèÈªòËÆ§ÂÄºÂàÜÂà´‰∏∫ **'glorot_uniform'** Âíå **'zeros'**„ÄÇ‰Ω†ÂèØ‰ª•Âú®Áõ∏Â∫îÁöÑ Keras [ÊñáÊ°£](https://keras.io/initializers/)‰∏≠ËØ¶ÁªÜ‰∫ÜËß£ÊØèÁßçÂàùÂßãÂåñÁ®ãÂ∫èÁöÑÂ∑•‰ΩúÊñπÊ≥ï„ÄÇ

Keras ‰∏≠ÊúâÂæàÂ§ö‰∏çÂêåÁöÑ[ÊçüÂ§±ÂáΩÊï∞](https://keras.io/losses/)„ÄÇÂØπ‰∫éËøôËäÇËØæÊù•ËØ¥ÔºåÊàë‰ª¨Â∞Ü‰ªÖ‰ΩøÁî® **categorical_crossentropy**„ÄÇ

ÂèÇÈòÖ Keras ‰∏≠ÂèØ[Áî®ÁöÑ‰ºòÂåñÁ®ãÂ∫èÂàóË°®](https://keras.io/optimizers/)„ÄÇÂΩì‰Ω†ÁºñËØëÊ®°ÂûãÔºàÂú®ËÆ∞‰∫ãÊú¨ÁöÑÁ¨¨ 7 Ê≠•ÔºâÊó∂Â∞±‰ºöÊåáÂÆö‰ºòÂåñÁ®ãÂ∫è„ÄÇ
>**'sgd'** : SGD

>**'rmsprop'** : RMSprop

>**'adagrad'** : Adagrad

>**'adadelta'** : Adadelta

>**'adam'** : Adam

>**'adamax'** : Adamax

>**'nadam'** : Nadam

>**'tfoptimizer'** : TFOptimizer

**ÂÖ≥‰∫éÊøÄÊ¥ªÂáΩÊï∞ÁöÑ[ÊñáÊ°£](http://cs231n.github.io/neural-networks-1/#actfun)**

# checkpoint

*Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠Ôºå‰Ω†ÂèØ‰ª•‰ΩøÁî®ÂæàÂ§öÂõûË∞ÉÔºà‰æãÂ¶Ç ModelCheckpointÔºâÊù•ÁõëÊéß‰Ω†ÁöÑÊ®°Âûã„ÄÇ‰Ω†ÂèØ‰ª•ÂèÇÈòÖÊ≠§Â§ÑÁöÑ[ËØ¶ÊÉÖÂÜÖÂÆπ](https://keras.io/callbacks/#modelcheckpoint)„ÄÇÂª∫ËÆÆ‰Ω†ÂÖàËØ¶ÁªÜ‰∫ÜËß£ EarlyStopping ÂõûË∞É„ÄÇÂ¶ÇÊûú‰Ω†ÊÉ≥Êü•ÁúãÂè¶‰∏Ä‰∏™ ModelCheckpoint ‰ª£Á†ÅÁ§∫‰æãÔºåËØ∑ÂèÇÈòÖ[ËøôÁØáÂçöÊñá](http://machinelearningmastery.com/check-point-deep-learning-models-keras/)„ÄÇ

*Mnist ÂèÇÈòÖ[ÂÖ∂‰ªñÂàÜÁ±ªÂô®](http://yann.lecun.com/exdb/mnist/)ÁöÑÊïàÊûú

# Ê±†Âåñ

ËØ∑ÂèÇÈòÖËØ• Keras [ÊñáÊ°£](https://keras.io/layers/pooling/)Ôºå‰∫ÜËß£‰∏çÂêåÁ±ªÂûãÁöÑÊ±†ÂåñÂ±ÇÔºÅ

ËÆ∫Êñá[network in network](https://arxiv.org/abs/1312.4400)

ÂèÇÈòÖ CIFAR-10 Á´ûËµõÁöÑ[Ëé∑ËÉúÊû∂ÊûÑ](http://blog.kaggle.com/2015/01/02/cifar-10-competition-winners-interviews-with-dr-ben-graham-phil-culliton-zygmunt-zajac/)ÔºÅ

# Êï∞ÊçÆÂ¢ûÂº∫

ÂÖ≥‰∫é `steps_per_epoch` ÁöÑÊ≥®ÊÑè‰∫ãÈ°π

`fit_generator` ÂÖ∑ÊúâÂæàÂ§öÂèÇÊï∞ÔºåÂåÖÊã¨

``` python
steps_per_epoch = x_train.shape[0] / batch_size
```
ÂÖ∂‰∏≠ `x_train.shape[0]` ÂØπÂ∫îÁöÑÊòØËÆ≠ÁªÉÊï∞ÊçÆÈõÜ x_train ‰∏≠ÁöÑÁã¨ÁâπÊ†∑Êú¨Êï∞Èáè„ÄÇÈÄöËøáÂ∞Ü steps_per_epoch ËÆæ‰∏∫Ê≠§ÂÄºÔºåÊàë‰ª¨Á°Æ‰øùÊ®°ÂûãÂú®ÊØè‰∏™ epoch ‰∏≠ÁúãÂà∞ `x_train.shape[0]` ‰∏™Â¢ûÂº∫ÂõæÁâá„ÄÇ



>ÈòÖËØªËøôÁØáÂØπ MNIST Êï∞ÊçÆÈõÜËøõË°åÂèØËßÜÂåñÁöÑ[Á≤æÂΩ©ÂçöÊñá](http://machinelearningmastery.com/image-augmentation-deep-learning-keras/)„ÄÇ

>ÂèÇÈòÖÊ≠§[ËØ¶ÁªÜÂÆûÁé∞](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)Ôºå‰∫ÜËß£Â¶Ç‰Ωï‰ΩøÁî®Â¢ûÂº∫ÂäüËÉΩÊèêÈ´ò Kaggle Êï∞ÊçÆÈõÜÁöÑÊïàÊûú„ÄÇ

>ÈòÖËØªÂÖ≥‰∫é ImageDataGenerator Á±ªÁöÑ Keras [ÊñáÊ°£](https://keras.io/preprocessing/image/)„ÄÇ

# Ë°•ÂÖÖËµÑÊñô

ÂèÇÈòÖ [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) ËÆ∫ÊñáÔºÅ

Âú®Ê≠§Â§ÑËØ¶ÁªÜ‰∫ÜËß£ [VGGNet](https://arxiv.org/pdf/1409.1556.pdf)„ÄÇ

Ê≠§Â§ÑÊòØ [ResNet](https://arxiv.org/pdf/1512.03385v1.pdf) ËÆ∫Êñá„ÄÇ

ËøôÊòØÁî®‰∫éËÆøÈóÆ‰∏Ä‰∫õËëóÂêç CNN Êû∂ÊûÑÁöÑ Keras [ÊñáÊ°£](https://keras.io/applications/)„ÄÇ

ÈòÖËØªËøô‰∏ÄÂÖ≥‰∫éÊ¢ØÂ∫¶Ê∂àÂ§±ÈóÆÈ¢òÁöÑ[ËØ¶ÁªÜÂ§ÑÁêÜÊñπÊ°à](http://neuralnetworksanddeeplearning.com/chap5.html)„ÄÇ

ËøôÊòØÂåÖÂê´‰∏çÂêå CNN Êû∂ÊûÑÁöÑÂü∫ÂáÜÁöÑ GitHub [ËµÑÊ∫êÂ∫ì](https://github.com/jcjohnson/cnn-benchmarks)„ÄÇ

ËÆøÈóÆ [ImageNet Large Scale Visual Recognition Competition (ILSVRC)](http://www.image-net.org/challenges/LSVRC/) ÁΩëÁ´ô„ÄÇ

ÂèØ‰ª•Âú®[Ê≠§Â§Ñ](https://github.com/udacity/machine-learning/tree/master/projects/practice_projects/cnn)ÈìæÊé•ÁöÑ GitHub ËµÑÊ∫êÂ∫ì‰∏≠ËÆøÈóÆËßÜÈ¢ë‰∏≠ÊèêÂà∞ÁöÑ Jupyter Notebook„ÄÇËΩ¨Âà∞ transfer-learning/ Êñá‰ª∂Â§πÂπ∂ÊâìÂºÄ transfer_learning.ipynb„ÄÇÂ¶ÇÊûú‰Ω†ÊÉ≥‰∫ÜËß£Â¶Ç‰ΩïËÆ°ÁÆóËá™Â∑±ÁöÑÁì∂È¢àÁâπÂæÅÔºåËØ∑Êü•Áúã bottleneck_features.ipynbÔºà‰Ω†ÂèØËÉΩÊó†Ê≥ïÂú® AWS GPU ÂÆû‰æã‰∏äËøêË°å bottleneck_features.ipynbÔºåÂ¶ÇÊûúÊòØËøôÁßçÊÉÖÂÜµÔºå‰Ω†ÂèØ‰ª•Âú®Êú¨Âú∞ CPU/GPU ‰∏ä‰ΩøÁî® notebookÔºÅÔºâ

ËØæÂ§ñËµÑÊñô
ËøôÊòØÊèêËÆÆÂ∞Ü GAP Â±ÇÁ∫ßÁî®‰∫éÂØπË±°ÂÆö‰ΩçÁöÑ[È¶ñÁØáÁ†îÁ©∂ËÆ∫Êñá](http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf)„ÄÇ
ÂèÇÈòÖËøô‰∏™‰ΩøÁî® CNN ËøõË°åÂØπË±°ÂÆö‰ΩçÁöÑ[ËµÑÊ∫êÂ∫ì](https://github.com/alexisbcook/ResNetCAM-keras)„ÄÇ
ËßÇÁúãËøô‰∏™ÂÖ≥‰∫é‰ΩøÁî® CNN ËøõË°åÂØπË±°ÂÆö‰ΩçÁöÑ[ËßÜÈ¢ëÊºîÁ§∫](https://www.youtube.com/watch?v=fZvOy0VXWAI)(YoutubeÈìæÊé•ÔºåÂõΩÂÜÖÁΩëÁªúÂèØËÉΩÊâì‰∏çÂºÄ)„ÄÇ
ÂèÇÈòÖËøô‰∏™‰ΩøÁî®ÂèØËßÜÂåñÊú∫Âô®Êõ¥Â•ΩÂú∞ÁêÜËß£Áì∂È¢àÁâπÂæÅÁöÑ[ËµÑÊ∫êÂ∫ì](https://github.com/alexisbcook/keras_transfer_cifar10)„ÄÇ

ÔºàÈùûÂ∏∏Ê£íÁöÑÔºâËØæÂ§ñËµÑÊñô ÔºÅ

Ê≥®ÔºöÁî±‰∫é‰ª•‰∏ãÈÉ®ÂàÜÈìæÊé•Êù•Ëá™‰∫éÂ§ñÁΩëÔºåÂõΩÂÜÖÁΩëÁªúÂèØËÉΩÊâì‰∏çÂºÄ

Â¶ÇÊûú‰Ω†ÊÉ≥ËØ¶ÁªÜ‰∫ÜËß£Â¶Ç‰ΩïËß£ËØª CNNÔºàÂ∞§ÂÖ∂ÊòØÂç∑ÁßØÂ±ÇÔºâÔºåÂª∫ËÆÆÊü•Áúã‰ª•‰∏ãËµÑÊñôÔºö

>ËøôÊòØÊëòËá™ÊñØÂù¶Á¶èÂ§ßÂ≠¶ÁöÑ CS231n ËØæÁ®ã‰∏≠ÁöÑ‰∏Ä‰∏™a [Á´†ËäÇ](http://cs231n.github.io/understanding-cnn/)ÔºåÂÖ∂‰∏≠ÂØπ CNN Â≠¶‰π†ÁöÑÂÜÖÂÆπËøõË°å‰∫ÜÂèØËßÜÂåñ„ÄÇ

>ÂèÇÈòÖËøô‰∏™ÂÖ≥‰∫éÂæàÈÖ∑ÁöÑ [OpenFrameworks](http://openframeworks.cc/) Â∫îÁî®ÁöÑ[ÊºîÁ§∫](https://aiexperiments.withgoogle.com/what-neural-nets-see)ÔºåËØ•Â∫îÁî®ÂèØ‰ª•Ê†πÊçÆÁî®Êà∑Êèê‰æõÁöÑËßÜÈ¢ëÂÆûÊó∂ÂèØËßÜÂåñ CNNÔºÅ

>ËøôÊòØÂè¶‰∏Ä‰∏™ CNN ÂèØËßÜÂåñÂ∑•ÂÖ∑ÁöÑ[ÊºîÁ§∫](https://www.youtube.com/watch?v=AgkfIQ4IGaM&t=78s)„ÄÇÂ¶ÇÊûú‰Ω†ÊÉ≥ËØ¶ÁªÜ‰∫ÜËß£Ëøô‰∫õÂèØËßÜÂåñÂõæË°®ÊòØÂ¶Ç‰ΩïÂà∂‰ΩúÁöÑÔºåËØ∑ËßÇÁúãÊ≠§[ËßÜÈ¢ë](https://www.youtube.com/watch?v=ghEmQSxT6tw&t=5s)„ÄÇ

>ËøôÊòØÂè¶‰∏Ä‰∏™ÂèØ‰∏é Keras Âíå Tensorflow ‰∏≠ÁöÑ CNN Êó†ÁºùÂêà‰ΩúÁöÑ[ÂèØËßÜÂåñÂ∑•ÂÖ∑](https://medium.com/merantix/picasso-a-free-open-source-visualizer-for-cnns-d8ed3a35cfc5)„ÄÇ

>ÈòÖËØªËøôÁØáÂèØËßÜÂåñ CNN Â¶Ç‰ΩïÁúãÂæÖËøô‰∏™‰∏ñÁïåÁöÑ [Keras ÂçöÊñá](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)„ÄÇÂú®Ê≠§ÂçöÊñá‰∏≠Ôºå‰Ω†‰ºöÊâæÂà∞ Deep Dreams ÁöÑÁÆÄÂçï‰ªãÁªçÔºå‰ª•ÂèäÂú® Keras ‰∏≠Ëá™Â∑±ÁºñÂÜô Deep Dreams ÁöÑ‰ª£Á†Å„ÄÇÈòÖËØª‰∫ÜËøôÁØáÂçöÊñáÂêéÔºö

>ÂÜçËßÇÁúãËøô‰∏™Âà©Áî® [Deep Dreams](https://www.youtube.com/watch?v=XatXy6ZhKZw) ÁöÑÈü≥‰πêËßÜÈ¢ëÔºàÊ≥®ÊÑè 3:15-3:40 ÈÉ®ÂàÜÔºâÔºÅ

>‰ΩøÁî®Ëøô‰∏™[ÁΩëÁ´ô](https://deepdreamgenerator.com/)ÂàõÂª∫Ëá™Â∑±ÁöÑ Deep DreamsÔºà‰∏çÁî®ÁºñÂÜô‰ªª‰Ωï‰ª£Á†ÅÔºÅÔºâ„ÄÇ

Â¶ÇÊûú‰Ω†ÊÉ≥ËØ¶ÁªÜ‰∫ÜËß£ CNN ÁöÑËß£Èáä

>ËøôÁØá[ÊñáÁ´†](https://blog.openai.com/adversarial-example-research/)ËØ¶ÁªÜËÆ≤Ëß£‰∫ÜÂú®Áé∞ÂÆûÁîüÊ¥ª‰∏≠‰ΩøÁî®Ê∑±Â∫¶Â≠¶‰π†Ê®°ÂûãÔºàÊöÇÊó∂Êó†Ê≥ïËß£ÈáäÔºâÁöÑ‰∏Ä‰∫õÂç±Èô©ÊÄß„ÄÇ

>Ëøô‰∏ÄÈ¢ÜÂüüÊúâÂæàÂ§öÁÉ≠ÁÇπÁ†îÁ©∂„ÄÇ[Ëøô‰∫õ‰ΩúËÄÖ](https://arxiv.org/abs/1611.03530)ÊúÄËøëÊúùÁùÄÊ≠£Á°ÆÁöÑÊñπÂêëËøàÂá∫‰∫Ü‰∏ÄÊ≠•„ÄÇ
