 *convolutional neural network summary æ˜¯ä¸€äº›å…³äºCNNçš„èµ„æ–™æ€»ç»“ã€‚

 *CNN_example æ˜¯åŸºäºç†è§£æ€»ç»“çš„ä¸€äº›CNNçš„åº”ç”¨å®ä¾‹

 *AI_doc æ˜¯ä¸€ä¸ªåº”ç”¨å®ä¾‹
 
 *AI_doc æ˜¯ä¸€ä¸ªtransfer learningåº”ç”¨å®ä¾‹
# Transfer Learning 
æ¨èä½ é˜…è¯»ä»¥ä¸‹ææ–™æ¥åŠ æ·±å¯¹ CNNå’ŒTransfer Learningçš„ç†è§£:

**[CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/)**

**[Using Convolutional Neural Networks to Classify Dog Breeds](http://cs231n.stanford.edu/reports/2015/pdfs/fcdh_FinalReport.pdf)**

**[Building an Image Classifier](https://towardsdatascience.com/learning-about-data-science-building-an-image-classifier-part-2-a7bcc6d5e825)**

**[Tips/Tricks in CNN](http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html)**

 - 1) data augmentation; 
 - 2) pre-processing on images; 
 - 3) initializations of Networks; 
 - 4) some tips during training; 
 - 5) selections of activation functions; 
 - 6) diverse regularizations; 
 - 7) some insights found from figures and finally 
 - 8) methods of ensemble multiple deep networks.

## [Transfer Learning using Keras](https://towardsdatascience.com/transfer-learning-using-keras-d804b2e04ef8)
1. **New dataset is small and similar to original dataset:**
There is a problem of over-fitting, if we try to train the entire network. Since the data is similar to the original data, we expect higher-level features in the ConvNet to be relevant to this dataset as well. Hence, the best idea might be to train a linear classifier on the CNN codes.

So lets freeze all the VGG19 layers and train only the classifier
```python
for layer in model.layers:
   layer.trainable = False
 
#Now we will be training only the classifiers (FC layers)
```
2. **New dataset is large and similar to the original dataset**
Since we have more data, we can have more confidence that we wonâ€™t overfit if we were to try to fine-tune through the full network.
``` python
for layer in model.layers:
   layer.trainable = True
#The default is already set to True. I have mentioned it here to make things clear.
```
In case if you want to freeze the first few layers as these layers will be detecting edges and blobs, you can freeze them by using the following code.
```python
for layer in model.layers[:5]:
   layer.trainable = False.
# Here I am freezing the first 5 layers 
```
3. **New dataset is small but very different from the original dataset**
Since the dataset is very small, We may want to extract the features from the earlier layer and train a classifier on top of that. This requires a little bit of knowledge on h5py.

The above code should help. It will extract the â€œblock2_poolâ€ features. In general this is not helpful as this layer has (64*64*128) features and training a classifier on top of it might not help us exactly. We can add a few FC layers and train a neural network on top of it. That should be straight forward.

*Add few FC layers and output layer.

*Set the weights for earlier layers and freeze them.

*Train the network.

4. **New dataset is large and very different from the original dataset.**
This is straight forward. since you have large dataset, you can design your own network or use the existing ones.

*Train the network using random initialisations or use the pre-trained network weights as initialisers. The second one is generally preferred.

*If you are using a different network or making small modification here and there for the existing network, Be careful with the naming conventions.

>[Transfer Learning in TensorFlow on the Kaggle Rainforest competition](https://medium.com/@luckylwk/transfer-learning-in-tensorflow-on-the-kaggle-rainforest-competition-4e978fadb571)

>[Transfer Learning and Fine-tuning](https://medium.com/deeplearningsandbox/how-to-use-transfer-learning-and-fine-tuning-in-keras-and-tensorflow-to-build-an-image-recognition-94b0b02444f2)

ç›¸å…³è®ºæ–‡:

**[[VGG16] VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION](https://arxiv.org/abs/1409.1556)**
- VGG16
``` python
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout, Flatten
from keras.layers import Conv2D
from keras.layers import MaxPooling2D

input_shape = (224, 224, 3)

model = Sequential([
    Conv2D(64, (3, 3), input_shape=input_shape, padding='same',
           activation='relu'),
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(128, (3, 3), activation='relu', padding='same'),
    Conv2D(128, (3, 3), activation='relu', padding='same',),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Flatten(),
    Dense(4096, activation='relu'),
    Dense(4096, activation='relu'),
    Dense(1000, activation='softmax')
])

model.summary()
```
- VGG19
``` python
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout, Flatten
from keras.layers import Conv2D
from keras.layers import MaxPooling2D

input_shape = (224, 224, 3)

model = Sequential([
    Conv2D(64, (3, 3), input_shape=input_shape, padding='same',
           activation='relu'),
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(128, (3, 3), activation='relu', padding='same'),
    Conv2D(128, (3, 3), activation='relu', padding='same',),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    Conv2D(256, (3, 3), activation='relu', padding='same',),
    Conv2D(256, (3, 3), activation='relu', padding='same',)
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',)
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',)
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Flatten(),
    Dense(4096, activation='relu'),
    Dense(4096, activation='relu'),
    Dense(1000, activation='softmax')
])

model.summary()
```

**[[Inception-v1] Going deeper with convolutions](https://arxiv.org/abs/1409.4842)**
è®²è§£-https://becominghuman.ai/understanding-and-coding-inception-module-in-keras-eb56e9056b4b
<div align=center><img width="550" src=resource/1.png></div>
``` python
#-*- coding: UTF-8 -*-

from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D
from keras.layers import Flatten, Dense, Dropout,BatchNormalization
from keras.layers import Input, concatenate
from keras.models import Model,load_model
from keras.preprocessing.image import ImageDataGenerator
from keras.utils import plot_model,np_utils
from keras import regularizers
import keras.metrics as metric
import os

# Global Constants
NB_CLASS=20
LEARNING_RATE=0.01
MOMENTUM=0.9
ALPHA=0.0001
BETA=0.75
GAMMA=0.1
DROPOUT=0.4
WEIGHT_DECAY=0.0005
LRN2D_NORM=True
DATA_FORMAT='channels_last' # Theano:'channels_first' Tensorflow:'channels_last'
USE_BN=True
IM_WIDTH=224
IM_HEIGHT=224
EPOCH=50

train_root='/home/faith/keras/dataset/traindata/'
vaildation_root='/home/faith/keras/dataset/vaildationdata/'
test_root='/home/faith/keras/dataset/testdata/'

IM_WIDTH=224
IM_HEIGHT=224
batch_size=32

#train data
train_datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    featurewise_center=True
)
train_generator = train_datagen.flow_from_directory(
  train_root,
  target_size=(IM_WIDTH, IM_HEIGHT),
  batch_size=batch_size,
)

#vaild data
vaild_datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    featurewise_center=True
)
vaild_generator = train_datagen.flow_from_directory(
  vaildation_root,
  target_size=(IM_WIDTH, IM_HEIGHT),
  batch_size=batch_size,
)

#test data
test_datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    featurewise_center=True
)
test_generator = train_datagen.flow_from_directory(
  test_root,
  target_size=(IM_WIDTH, IM_HEIGHT),
  batch_size=batch_size,
)

#normalization
def conv2D_lrn2d(x,filters,kernel_size,strides=(1,1),padding='same',data_format=DATA_FORMAT,dilation_rate=(1,1),activation='relu',use_bias=True,kernel_initializer='glorot_uniform',bias_initializer='zeros',kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,lrn2d_norm=LRN2D_NORM,weight_decay=WEIGHT_DECAY):
    #l2 normalization
    if weight_decay:
        kernel_regularizer=regularizers.l2(weight_decay)
        bias_regularizer=regularizers.l2(weight_decay)
    else:
        kernel_regularizer=None
        bias_regularizer=None

    x=Conv2D(filters=filters,kernel_size=kernel_size,strides=strides,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)

    if lrn2d_norm:
        #batch normalization
        x=BatchNormalization()(x)

    return x



def inception_module(x,params,concat_axis,padding='same',data_format=DATA_FORMAT,dilation_rate=(1,1),activation='relu',use_bias=True,kernel_initializer='glorot_uniform',bias_initializer='zeros',kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,lrn2d_norm=LRN2D_NORM,weight_decay=None):
    (branch1,branch2,branch3,branch4)=params
    if weight_decay:
        kernel_regularizer=regularizers.l2(weight_decay)
        bias_regularizer=regularizers.l2(weight_decay)
    else:
        kernel_regularizer=None
        bias_regularizer=None
    #1x1
    pathway1=Conv2D(filters=branch1[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)

    #1x1->3x3
    pathway2=Conv2D(filters=branch2[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)
    pathway2=Conv2D(filters=branch2[1],kernel_size=(3,3),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway2)

    #1x1->5x5
    pathway3=Conv2D(filters=branch3[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(x)
    pathway3=Conv2D(filters=branch3[1],kernel_size=(5,5),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway3)

    #3x3->1x1
    pathway4=MaxPooling2D(pool_size=(3,3),strides=1,padding=padding,data_format=DATA_FORMAT)(x)
    pathway4=Conv2D(filters=branch4[0],kernel_size=(1,1),strides=1,padding=padding,data_format=data_format,dilation_rate=dilation_rate,activation=activation,use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,bias_constraint=bias_constraint)(pathway4)

    return concatenate([pathway1,pathway2,pathway3,pathway4],axis=concat_axis)



def create_model():
    #Data format:tensorflow,channels_last;theano,channels_last
    if DATA_FORMAT=='channels_first':
        INP_SHAPE=(3,224,224)
        img_input=Input(shape=INP_SHAPE)
        CONCAT_AXIS=1
    elif DATA_FORMAT=='channels_last':
        INP_SHAPE=(224,224,3)
        img_input=Input(shape=INP_SHAPE)
        CONCAT_AXIS=3
    else:
        raise Exception('Invalid Dim Ordering')

    x=conv2D_lrn2d(img_input,64,(7,7),2,padding='same',lrn2d_norm=False)
    x=MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x)
    x=BatchNormalization()(x)

    x=conv2D_lrn2d(x,64,(1,1),1,padding='same',lrn2d_norm=False)

    x=conv2D_lrn2d(x,192,(3,3),1,padding='same',lrn2d_norm=True)
    x=MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x)

    x=inception_module(x,params=[(64,),(96,128),(16,32),(32,)],concat_axis=CONCAT_AXIS) #3a
    x=inception_module(x,params=[(128,),(128,192),(32,96),(64,)],concat_axis=CONCAT_AXIS) #3b
    x=MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x)

    x=inception_module(x,params=[(192,),(96,208),(16,48),(64,)],concat_axis=CONCAT_AXIS) #4a
    x=inception_module(x,params=[(160,),(112,224),(24,64),(64,)],concat_axis=CONCAT_AXIS) #4b
    x=inception_module(x,params=[(128,),(128,256),(24,64),(64,)],concat_axis=CONCAT_AXIS) #4c
    x=inception_module(x,params=[(112,),(144,288),(32,64),(64,)],concat_axis=CONCAT_AXIS) #4d
    x=inception_module(x,params=[(256,),(160,320),(32,128),(128,)],concat_axis=CONCAT_AXIS) #4e
    x=MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x)

    x=inception_module(x,params=[(256,),(160,320),(32,128),(128,)],concat_axis=CONCAT_AXIS) #5a
    x=inception_module(x,params=[(384,),(192,384),(48,128),(128,)],concat_axis=CONCAT_AXIS) #5b
    x=AveragePooling2D(pool_size=(7,7),strides=1,padding='valid',data_format=DATA_FORMAT)(x)

    x=Flatten()(x)
    x=Dropout(DROPOUT)(x)
    x=Dense(output_dim=NB_CLASS,activation='linear')(x)
    x=Dense(output_dim=NB_CLASS,activation='softmax')(x)

    return x,img_input,CONCAT_AXIS,INP_SHAPE,DATA_FORMAT


def check_print():
    # Create the Model
    x,img_input,CONCAT_AXIS,INP_SHAPE,DATA_FORMAT=create_model()

    # Create a Keras Model
    model=Model(input=img_input,output=[x])
    model.summary()

    # Save a PNG of the Model Build
    plot_model(model,to_file='GoogLeNet.png')

    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc',metric.top_k_categorical_accuracy])
    print 'Model Compiled'
    return model

if __name__=='__main__':
    if os.path.exists('inception_1.h5'):
        model=load_model('inception_1.h5')
    else:
        model=check_print()

    model.fit_generator(train_generator,validation_data=vaild_generator,epochs=EPOCH,steps_per_epoch=train_generator.n/batch_size
                        ,validation_steps=vaild_generator.n/batch_size)
    model.save('inception_1.h5')
    model.metrics=['acc',metric.top_k_categorical_accuracy]
    loss,acc,top_acc=model.evaluate_generator(test_generator,steps=test_generator.n/batch_size)
    print 'Test result:loss:%f,acc:%f,top_acc:%f'%(loss,acc,top_acc)
--------------------- 
ä½œè€…ï¼šSpongelady 
æ¥æºï¼šCSDN 
åŸæ–‡ï¼šhttps://blog.csdn.net/qq_25491201/article/details/78367696 
ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºåšä¸»åŸåˆ›æ–‡ç« ï¼Œè½¬è½½è¯·é™„ä¸Šåšæ–‡é“¾æ¥ï¼
```

>[[Inception-v3] Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567)

>[[Inception-v4] Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/abs/1602.07261)

>[[ResNet] Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)

>[[Xception] Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357)



æ­¤å¤–ï¼Œæˆ‘æ³¨æ„åˆ°ä½ ä½¿ç”¨çš„å›¾ç‰‡æ˜¯ä»æˆ‘ä»¬æä¾›çš„æ•°æ®é›†ä¸­é€‰å–çš„ï¼Œæˆ‘éå¸¸ä¸æ¨èè¿™ç§åšæ³•ã€‚è¿™ç§è¡Œä¸ºå¯èƒ½ä¼šå¯¼è‡´æ ‡ç­¾æ³„éœ²ï¼ˆLabel Leakageï¼‰ï¼Œå¹¶ä¸èƒ½å¾ˆå¥½çš„è¯„ä¼°ä½ çš„æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å› ä¸ºï¼Œæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ¬èº«å°±æ˜¯åœ¨ä¸æ–­æ‹Ÿåˆè®­ç»ƒé›†ï¼Œå®ƒèƒ½å¾ˆå¥½åœ°é¢„æµ‹è®­ç»ƒé›†é‡Œçš„å›¾ç‰‡æ˜¯ç†æ‰€åº”å½“çš„ã€‚è€ŒéªŒè¯æ³›åŒ–èƒ½åŠ›æœ€å¥½çš„åšæ³•å°±æ˜¯ï¼Œä½¿ç”¨çœŸå®çš„ã€åœ¨è®­ç»ƒé›†/æµ‹è¯•é›†/éªŒè¯é›†éƒ½æ²¡æœ‰å‡ºç°è¿‡çš„å›¾ç‰‡æ¥è¿›è¡Œæµ‹è¯•ã€‚ä½ å¯ä»¥è‡ªç”±çš„ä½¿ç”¨ç½‘ä¸Šçš„å›¾ç‰‡æˆ–è€…è‡ªå·±çš„å›¾ç‰‡~ğŸ˜‰ åŒæ—¶ï¼Œå¸Œæœ›ä½ èƒ½å°è¯•ç±»å‹çš„å›¾ç‰‡æ¥è¿›è¡Œå®éªŒï¼Œæ¯”å¦‚çŒ«ã€å¤šæ¡ç‹—ï¼ˆå¯ä»¥æ˜¯ä¸åŒå“ç§ï¼‰ã€å¸¦ç€ç‹—è€³æœµçš„äººã€é£æ™¯ç…§ç­‰ã€‚æŒ‰ç…§æœºå™¨å­¦ä¹ çš„æ€è·¯ï¼Œä½ çš„è¾“å…¥è¦†ç›–çš„è¾“å…¥ç©ºé—´è¶Šå¤šï¼Œé‚£ä¹ˆä½ å°±èƒ½å¯¹æ¨¡å‹è¿›è¡Œè¶Šå¥½çš„è¯„ä¼°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä½ å°è¯•çš„å›¾ç‰‡ç±»å‹è¶Šå¤šï¼Œå¯¹æ¨¡å‹çš„è¯„ä¼°èƒ½åŠ›å°±è¶Šå¼ºã€‚ğŸ˜„

ä»¥ä¸‹æ˜¯æˆ‘å¯¹æ”¹è¿›æ¨¡å‹æå‡ºçš„å»ºè®®ï¼Œå¸Œæœ›å¯¹ä½ æœ‰å¸®åŠ©ï¼š

1.äº¤å‰éªŒè¯ï¼ˆCross Validationï¼‰ åœ¨æœ¬æ¬¡è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬åªè¿›è¡Œäº†ä¸€æ¬¡è®­ç»ƒé›†/æµ‹è¯•é›†åˆ‡åˆ†ï¼Œè€Œåœ¨å®é™…æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¾€å¾€æ˜¯ä½¿ç”¨äº¤å‰éªŒè¯ï¼ˆCross Validationï¼‰æ¥è¿›è¡Œæ¨¡å‹é€‰æ‹©ï¼ˆModel Selectionï¼‰å’Œè°ƒå‚ï¼ˆParameter Tunningï¼‰çš„ã€‚äº¤å‰éªŒè¯çš„é€šå¸¸åšæ³•æ˜¯ï¼ŒæŒ‰ç…§æŸç§æ–¹å¼å¤šæ¬¡è¿›è¡Œè®­ç»ƒé›†/æµ‹è¯•é›†åˆ‡åˆ†ï¼Œæœ€ç»ˆå–å¹³å‡å€¼ï¼ˆåŠ æƒå¹³å‡å€¼ï¼‰ï¼Œå…·ä½“å¯ä»¥å‚è€ƒç»´åŸºç™¾ç§‘)çš„ä»‹ç»ã€‚

2.æ¨¡å‹èåˆ/é›†æˆå­¦ä¹ ï¼ˆModel Ensemblingï¼‰ é€šè¿‡åˆ©ç”¨ä¸€äº›æœºå™¨å­¦ä¹ ä¸­æ¨¡å‹èåˆçš„æŠ€æœ¯ï¼Œå¦‚votingã€baggingã€blendingä»¥åŠstakingç­‰ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„å‡†ç¡®ç‡ä¸é²æ£’æ€§ï¼Œä¸”å‡ ä¹æ²¡æœ‰é£é™©ã€‚ä½ å¯ä»¥å‚è€ƒæˆ‘æ•´ç†çš„æœºå™¨å­¦ä¹ ç¬”è®°ä¸­çš„Ensembleéƒ¨åˆ†ã€‚

3.æ›´å¤šçš„æ•°æ® å¯¹äºæ·±åº¦å­¦ä¹ ï¼ˆæœºå™¨å­¦ä¹ ï¼‰ä»»åŠ¡æ¥è¯´ï¼Œæ›´å¤šçš„æ•°æ®æ„å‘³ç€æ›´ä¸ºä¸°å¯Œçš„è¾“å…¥ç©ºé—´ï¼Œå¯ä»¥å¸¦æ¥æ›´å¥½çš„è®­ç»ƒæ•ˆæœã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æ•°æ®å¢å¼ºï¼ˆData Augmentationï¼‰ã€å¯¹æŠ—ç”Ÿæˆç½‘ç»œï¼ˆGenerative Adversarial Networksï¼‰ç­‰æ–¹å¼æ¥å¯¹æ•°æ®é›†è¿›è¡Œæ‰©å……ï¼ŒåŒæ—¶è¿™ç§æ–¹å¼ä¹Ÿèƒ½æå‡æ¨¡å‹çš„é²æ£’æ€§ã€‚

4.æ›´æ¢äººè„¸æ£€æµ‹ç®—æ³• å°½ç®¡OpenCVå·¥å…·åŒ…éå¸¸æ–¹ä¾¿å¹¶ä¸”é«˜æ•ˆï¼ŒHaarçº§è”æ£€æµ‹ä¹Ÿæ˜¯ä¸€ä¸ªå¯ä»¥ç›´æ¥ä½¿ç”¨çš„å¼ºåŠ›ç®—æ³•ï¼Œä½†æ˜¯è¿™äº›ç®—æ³•ä»ç„¶ä¸èƒ½è·å¾—å¾ˆé«˜çš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸”éœ€è¦ç”¨æˆ·æä¾›æ­£é¢ç…§ç‰‡ï¼Œè¿™å¸¦æ¥çš„ä¸€å®šçš„ä¸ä¾¿ã€‚æ‰€ä»¥å¦‚æœæƒ³è¦è·å¾—æ›´å¥½çš„ç”¨æˆ·ä½“éªŒå’Œå‡†ç¡®ç‡ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä¸€äº›æ–°çš„äººè„¸è¯†åˆ«ç®—æ³•ï¼Œå¦‚åŸºäºæ·±åº¦å­¦ä¹ çš„ä¸€äº›ç®—æ³•ã€‚

5.å¤šç›®æ ‡ç›‘æµ‹ æ›´è¿›ä¸€æ­¥ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€äº›å…ˆè¿›çš„ç›®æ ‡è¯†åˆ«ç®—æ³•ï¼Œå¦‚RCNNã€Fast-RCNNã€Faster-RCNNæˆ–Masked-RCNNç­‰ï¼Œæ¥å®Œæˆä¸€å¼ ç…§ç‰‡ä¸­åŒæ—¶å‡ºç°å¤šä¸ªç›®æ ‡çš„æ£€æµ‹ä»»åŠ¡ã€‚
