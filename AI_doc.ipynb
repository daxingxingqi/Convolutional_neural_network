{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**sklearn.datasets.load_files**](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html#sklearn.datasets.load_files)\n",
    "\n",
    "\n",
    "sklearn.datasets.load_files(container_path, description=None, categories=None, load_content=True, shuffle=True, encoding=None, decode_error=’strict’, random_state=0\n",
    "\n",
    "container_folder/\n",
    "\n",
    "    category_1_folder/\n",
    "    \n",
    "        file_1.txt file_2.txt … file_42.txt\n",
    "        \n",
    "    category_2_folder/\n",
    "    \n",
    "        file_43.txt file_44.txt …\n",
    "        \n",
    "The folder names are used as supervised signal label names. The individual file names are not important.\n",
    "\n",
    "This function does not try to extract features into a numpy array or scipy sparse matrix. In addition, if load_content is false it does not try to load the files in memory.\n",
    "\n",
    "To use text files in a scikit-learn classification or clustering algorithm, you will need to use the sklearn.feature_extraction.text module to build a feature extraction transformer that suits your problem.\n",
    "\n",
    "If you set load_content=True, you should also specify the encoding of the text using the ‘encoding’ parameter. For many modern text files, ‘utf-8’ will be the correct encoding. If you leave encoding equal to None, then the content will be made of bytes instead of Unicode, and you will not be able to use most functions in sklearn.feature_extraction.text.\n",
    "\n",
    "Similar feature extractors should be built for other kind of unstructured data input such as images, audio, video, …\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**to_categorical**](https://keras.io/utils/)\n",
    "\n",
    "keras.utils.to_categorical(y, num_classes=None, dtype='float32')\n",
    "Converts a class vector (integers) to binary class matrix.\n",
    "\n",
    "E.g. for use with categorical_crossentropy.\n",
    "\n",
    "Arguments\n",
    "\n",
    ">y: class vector to be converted into a matrix (integers from 0 to num_classes).\n",
    "\n",
    ">num_classes: total number of classes.\n",
    "\n",
    ">dtype: The data type expected by the input, as a string (float32, float64,  int32...)\n",
    "Returns\n",
    "\n",
    "A binary matrix representation of the input. The classes axis is placed last.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 total disease categories.\n",
      "There are 2750 total disease images.\n",
      "\n",
      "There are 2000 training disease images.\n",
      "There are 150 validation disease images.\n",
      "There are 600 test disease images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "#define function load files acoording to subfolders, one-hot labels\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    diseases_files = np.array(data['filenames'])\n",
    "    diseases_targets = np_utils.to_categorical(np.array(data['target']), 3)\n",
    "    return diseases_files, diseases_targets\n",
    "\n",
    "train_files, train_targets = load_dataset('./data/train')\n",
    "valid_files, valid_targets = load_dataset('./data/valid')\n",
    "test_files, test_targets = load_dataset('./data/test')\n",
    "\n",
    "#diseases_names =  [item for item in sorted(glob('./data/train/*/'))]\n",
    "\n",
    "print('There are %d total disease categories.' % len(diseases_names))\n",
    "print('There are %s total disease images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training disease images.' % len(train_files))\n",
    "print('There are %d validation disease images.' % len(valid_files))\n",
    "print('There are %d test disease images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases_names =  [item[13:-1] for item in sorted(glob('./data/train/*/'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['melanoma', 'nevus', 'seborrheic_keratosis']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diseases_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(20)\n",
    "random.shuffle(train_files)\n",
    "random.shuffle(valid_files)\n",
    "random.shuffle(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['./data/train/melanoma/ISIC_0015110.jpg',\n",
       "       './data/train/seborrheic_keratosis/ISIC_0014642.jpg',\n",
       "       './data/train/melanoma/ISIC_0000551.jpg', ...,\n",
       "       './data/train/nevus/ISIC_0012164.jpg',\n",
       "       './data/train/nevus/ISIC_0014516.jpg',\n",
       "       './data/train/melanoma/ISIC_0000517.jpg'], dtype='<U50')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    img = image.load_img(img_path, target_size = (224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look around line 220 (in your case line 201—perhaps you are running a slightly different version), we see that PIL is reading in blocks of the file and that it expects that the blocks are going to be of a certain size. It turns out that you can ask PIL to be tolerant of files that are truncated (missing some file from the block) by changing a setting.\n",
    "\n",
    "Somewhere before your code block, simply add the following:\n",
    "``` python\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:45<00:00, 18.88it/s]\n",
      "100%|██████████| 150/150 [00:14<00:00, 10.25it/s]\n",
      "100%|██████████| 600/600 [01:26<00:00,  6.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 224, 224, 64)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 112, 112, 32)      18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 112, 112, 32)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 16)        4624      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 56, 56, 16)        224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 26,499\n",
      "Trainable params: 25,715\n",
      "Non-trainable params: 784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "### TODO: 定义你的网络架构\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu', input_shape=(224, 224, 3)))   \n",
    "model.add(BatchNormalization(axis = 1 ))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization(axis = 1 ))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization(axis = 1 ))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 编译模型\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 150 samples\n",
      "Epoch 1/150\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8402 - acc: 0.6860 - val_loss: 1.1257 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.12568, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/150\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8389 - acc: 0.6860 - val_loss: 1.0898 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.12568 to 1.08985, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/150\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8403 - acc: 0.6860 - val_loss: 1.0449 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.08985 to 1.04490, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 4/150\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8372 - acc: 0.6860 - val_loss: 1.0410 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.04490 to 1.04097, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 5/150\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8375 - acc: 0.6860 - val_loss: 1.0859 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.04097\n",
      "Epoch 6/150\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8357 - acc: 0.6860 - val_loss: 1.0348 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.04097 to 1.03481, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 7/150\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8342 - acc: 0.6860 - val_loss: 1.1220 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.03481\n",
      "Epoch 8/150\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8407 - acc: 0.6860 - val_loss: 1.0822 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.03481\n",
      "Epoch 9/150\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8370 - acc: 0.6860 - val_loss: 1.0725 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.03481\n",
      "Epoch 10/150\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8346 - acc: 0.6860 - val_loss: 1.0647 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.03481\n",
      "Epoch 11/150\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8359 - acc: 0.6860 - val_loss: 1.0570 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.03481\n",
      "Epoch 12/150\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8337 - acc: 0.6860 - val_loss: 1.0602 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.03481\n",
      "Epoch 13/150\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8357 - acc: 0.6860 - val_loss: 1.0476 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.03481\n",
      "Epoch 14/150\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8352 - acc: 0.6860 - val_loss: 1.0603 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.03481\n",
      "Epoch 15/150\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8351 - acc: 0.6860 - val_loss: 1.1027 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.03481\n",
      "Epoch 16/150\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.8333 - acc: 0.6860 - val_loss: 1.0403 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.03481\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f332474aef0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "### 不要修改下方代码\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5',  monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience = 10, verbose = 1)\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer, earlystopper], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载具有最好验证loss的模型\n",
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 65.5000%\n"
     ]
    }
   ],
   "source": [
    "# 获取测试数据集中每一个图像所预测的狗品种的index\n",
    "predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# 报告测试准确率\n",
    "test_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      " - 12s - loss: 0.8331 - acc: 0.6860 - val_loss: 1.1215 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.12152, saving model to saved_models/aug_model.weights.best.hdf5\n",
      "Epoch 2/150\n",
      " - 11s - loss: 0.8345 - acc: 0.6860 - val_loss: 1.0794 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.12152 to 1.07944, saving model to saved_models/aug_model.weights.best.hdf5\n",
      "Epoch 3/150\n",
      " - 11s - loss: 0.8349 - acc: 0.6860 - val_loss: 1.1079 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.07944\n",
      "Epoch 4/150\n",
      " - 11s - loss: 0.8344 - acc: 0.6860 - val_loss: 1.1303 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.07944\n",
      "Epoch 5/150\n",
      " - 11s - loss: 0.8337 - acc: 0.6860 - val_loss: 1.0520 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.07944 to 1.05203, saving model to saved_models/aug_model.weights.best.hdf5\n",
      "Epoch 6/150\n",
      " - 12s - loss: 0.8352 - acc: 0.6860 - val_loss: 1.0960 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.05203\n",
      "Epoch 7/150\n",
      " - 11s - loss: 0.8348 - acc: 0.6860 - val_loss: 1.1271 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.05203\n",
      "Epoch 8/150\n",
      " - 11s - loss: 0.8341 - acc: 0.6860 - val_loss: 1.0703 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.05203\n",
      "Epoch 9/150\n",
      " - 11s - loss: 0.8348 - acc: 0.6860 - val_loss: 1.0834 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.05203\n",
      "Epoch 10/150\n",
      " - 11s - loss: 0.8333 - acc: 0.6860 - val_loss: 1.0778 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.05203\n",
      "Epoch 11/150\n",
      " - 11s - loss: 0.8334 - acc: 0.6860 - val_loss: 1.0826 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.05203\n",
      "Epoch 12/150\n",
      " - 11s - loss: 0.8330 - acc: 0.6860 - val_loss: 1.1381 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.05203\n",
      "Epoch 13/150\n",
      " - 11s - loss: 0.8350 - acc: 0.6860 - val_loss: 1.0885 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.05203\n",
      "Epoch 14/150\n",
      " - 11s - loss: 0.8323 - acc: 0.6860 - val_loss: 1.1161 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.05203\n",
      "Epoch 15/150\n",
      " - 11s - loss: 0.8339 - acc: 0.6860 - val_loss: 1.1278 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.05203\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f332474ae10>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# create and configure augmented image generator\n",
    "datagen_train = ImageDataGenerator(\n",
    "    width_shift_range=0.5,  # randomly shift images horizontally (10% of total width)\n",
    "    height_shift_range=0.5,  # randomly shift images vertically (10% of total height)\n",
    "    horizontal_flip=True) # randomly flip images horizontally\n",
    "\n",
    "# fit augmented image generator on data\n",
    "datagen_train.fit(train_tensors)\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping  \n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience = 10, verbose = 1)\n",
    "batch_size = 20\n",
    "epochs = 150\n",
    "\n",
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/aug_model.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience = 10, verbose = 1)\n",
    "model.fit_generator(datagen_train.flow(train_tensors, train_targets, batch_size=batch_size),\n",
    "                    steps_per_epoch=train_tensors.shape[0] // batch_size,\n",
    "                    epochs=epochs, verbose=2, callbacks=[checkpointer, earlystopper],\n",
    "                    validation_data=(valid_tensors, valid_targets),\n",
    "                    validation_steps=valid_tensors.shape[0] // batch_size)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 65.5000%\n"
     ]
    }
   ],
   "source": [
    "## 加载具有最好验证loss的模型\n",
    "model.load_weights('saved_models/aug_model.weights.best.hdf5')\n",
    "# 获取测试数据集中每一个图像所预测的狗品种的index\n",
    "predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# 报告测试准确率\n",
    "test_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
