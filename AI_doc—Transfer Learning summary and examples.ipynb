{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**sklearn.datasets.load_files**](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html#sklearn.datasets.load_files)\n",
    "\n",
    "\n",
    "sklearn.datasets.load_files(container_path, description=None, categories=None, load_content=True, shuffle=True, encoding=None, decode_error=’strict’, random_state=0\n",
    "\n",
    "container_folder/\n",
    "\n",
    "    category_1_folder/\n",
    "    \n",
    "        file_1.txt file_2.txt … file_42.txt\n",
    "        \n",
    "    category_2_folder/\n",
    "    \n",
    "        file_43.txt file_44.txt …\n",
    "        \n",
    "The folder names are used as supervised signal label names. The individual file names are not important.\n",
    "\n",
    "This function does not try to extract features into a numpy array or scipy sparse matrix. In addition, if load_content is false it does not try to load the files in memory.\n",
    "\n",
    "To use text files in a scikit-learn classification or clustering algorithm, you will need to use the sklearn.feature_extraction.text module to build a feature extraction transformer that suits your problem.\n",
    "\n",
    "If you set load_content=True, you should also specify the encoding of the text using the ‘encoding’ parameter. For many modern text files, ‘utf-8’ will be the correct encoding. If you leave encoding equal to None, then the content will be made of bytes instead of Unicode, and you will not be able to use most functions in sklearn.feature_extraction.text.\n",
    "\n",
    "Similar feature extractors should be built for other kind of unstructured data input such as images, audio, video, …\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**to_categorical**](https://keras.io/utils/)\n",
    "\n",
    "keras.utils.to_categorical(y, num_classes=None, dtype='float32')\n",
    "Converts a class vector (integers) to binary class matrix.\n",
    "\n",
    "E.g. for use with categorical_crossentropy.\n",
    "\n",
    "Arguments\n",
    "\n",
    ">y: class vector to be converted into a matrix (integers from 0 to num_classes).\n",
    "\n",
    ">num_classes: total number of classes.\n",
    "\n",
    ">dtype: The data type expected by the input, as a string (float32, float64,  int32...)\n",
    "Returns\n",
    "\n",
    "A binary matrix representation of the input. The classes axis is placed last.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "#define function load files acoording to subfolders, one-hot labels\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    diseases_files = np.array(data['filenames'])\n",
    "    diseases_targets = np_utils.to_categorical(np.array(data['target']), 3)\n",
    "    return diseases_files, diseases_targets\n",
    "\n",
    "train_files, train_targets = load_dataset('./data/train')\n",
    "valid_files, valid_targets = load_dataset('./data/valid')\n",
    "test_files, test_targets = load_dataset('./data/test')\n",
    "\n",
    "#diseases_names =  [item for item in sorted(glob('./data/train/*/'))]\n",
    "\n",
    "#print('There are %d total disease categories.' % len(diseases_names))\n",
    "print('There are %s total disease images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training disease images.' % len(train_files))\n",
    "print('There are %d validation disease images.' % len(valid_files))\n",
    "print('There are %d test disease images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases_names =  [item[13:-1] for item in sorted(glob('./data/train/*/'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(20)\n",
    "random.shuffle(train_files)\n",
    "random.shuffle(valid_files)\n",
    "random.shuffle(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    img = image.load_img(img_path, target_size = (224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look around line 220 (in your case line 201—perhaps you are running a slightly different version), we see that PIL is reading in blocks of the file and that it expects that the blocks are going to be of a certain size. It turns out that you can ask PIL to be tolerant of files that are truncated (missing some file from the block) by changing a setting.\n",
    "\n",
    "Somewhere before your code block, simply add the following:\n",
    "``` python\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB\n",
    "\n",
    "Image Preprocession - (https://keras.io/preprocessing/image/)\n",
    "\n",
    "使用`flow`的话，输入的图片需要预先是tensor的模式（也就是四维张量）\n",
    "\n",
    "`flow`\n",
    "\n",
    "Returns\n",
    "\n",
    "An Iterator yielding tuples of (x, y) where x is a numpy array of image data (in the case of a single image input) or a list of numpy arrays (in the case with additional inputs) and y is a numpy array of corresponding labels. If 'sample_weight' is not None, the yielded tuples are of the form  (x, y, sample_weight). If y is None, only the numpy array x is returned.\n",
    "```python\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
    "                    steps_per_epoch=len(x_train) / 32, epochs=epochs)\n",
    "\n",
    "# here's a more \"manual\" example\n",
    "for e in range(epochs):\n",
    "    print('Epoch', e)\n",
    "    batches = 0\n",
    "    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):\n",
    "        model.fit(x_batch, y_batch)\n",
    "        batches += 1\n",
    "        if batches >= len(x_train) / 32:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "```\n",
    "`flow_from_directory`\n",
    "\n",
    "Returns\n",
    "\n",
    "A DirectoryIterator yielding tuples of (x, y) where x is a numpy array containing a batch of images with shape  (batch_size, *target_size, channels) and y is a numpy array of corresponding labels.\n",
    "``` python\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/train',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'data/validation',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New dataset is small and similar to original dataset and  New dataset is large and similar to the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB \n",
    "Keras functional API -(https://keras.io/getting-started/functional-api-guide/)\n",
    "\n",
    "Flatten-(https://stackoverflow.com/questions/43237124/role-of-flatten-in-keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout,Conv2D, Flatten, Dense, GlobalAveragePooling2D, Input\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "img_width, img_height = 256, 256\n",
    "train_data_dir = \"./data/train\"\n",
    "validation_data_dir = \"./data/valid\"\n",
    "nb_train_samples = 4216\n",
    "nb_validation_samples = 466\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "###  定义你的网络架构\n",
    "model = applications.VGG19(weights = \"imagenet\", include_top=False, input_shape = (img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
    "for layer in model.layers[:5]:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Adding custom Layers \n",
    "x = model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "predictions = Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "# creating the final model \n",
    "model_final = Model(input = model.input, output = predictions)\n",
    "model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB\n",
    "fit_generator-(https://keras.io/models/sequential/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# create and configure augmented image generator\n",
    "# Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "train_datagen.fit(train_tensors)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "train_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "batch_size = batch_size, \n",
    "class_mode = \"categorical\")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "validation_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "class_mode = \"categorical\")\n",
    "\n",
    "# Save the model according to the conditions  \n",
    "checkpoint = ModelCheckpoint(\"vgg19_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "\n",
    "# Train the model \n",
    "model_final.fit_generator(\n",
    "train_generator,\n",
    "samples_per_epoch = nb_train_samples,\n",
    "epochs = epochs,\n",
    "validation_data = validation_generator,\n",
    "nb_val_samples = nb_validation_samples,\n",
    "callbacks = [checkpoint, early])\n",
    "\n",
    "model_final.load_weights('vgg19_1.h5')\n",
    "# 获取测试数据集中每一个图像所预测的狗品种的index\n",
    "predictions = [np.argmax(model_final.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# 报告测试准确率\n",
    "test_accuracy = 100*np.sum(np.array(predictions)==np.argmax(test_targets, axis=1))/len(predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Epoch 1/150\n",
    " - 12s - loss: 0.8331 - acc: 0.6860 - val_loss: 1.1215 - val_acc: 0.5200\n",
    "\n",
    "Epoch 00001: val_loss improved from inf to 1.12152, saving model to saved_models/aug_model.weights.best.hdf5\n",
    "Epoch 2/150\n",
    " - 11s - loss: 0.8345 - acc: 0.6860 - val_loss: 1.0794 - val_acc: 0.5200\n",
    "\n",
    "Epoch 00002: val_loss improved from 1.12152 to 1.07944, saving model to saved_models/aug_model.weights.best.hdf5\n",
    "Epoch 3/150\n",
    " - 11s - loss: 0.8349 - acc: 0.6860 - val_loss: 1.1079 - val_acc: 0.5200\n",
    "\n",
    "Epoch 00003: val_loss did not improve from 1.07944\n",
    "Epoch 4/150\n",
    " - 11s - loss: 0.8344 - acc: 0.6860 - val_loss: 1.1303 - val_acc: 0.5200\n",
    "\n",
    "Epoch 00004: val_loss did not improve from 1.07944\n",
    "Epoch 5/150\n",
    " - 11s - loss: 0.8337 - acc: 0.6860 - val_loss: 1.0520 - val_acc: 0.5200\n",
    "\n",
    "Epoch 00005: val_loss improved from 1.07944 to 1.05203, saving model to saved_models/aug_model.weights.best.hdf5\n",
    "Epoch 6/150\n",
    " - 12s - loss: 0.8352 - acc: 0.6860 - val_loss: 1.0960 - val_acc: 0.5200\n",
    "\n",
    "Epoch 00006: val_loss did not improve from 1.05203\n",
    "Epoch 7/150\n",
    " - 11s - loss: 0.8348 - acc: 0.6860 - val_loss: 1.1271 - val_acc: 0.5200\n",
    "\n",
    "Epoch 00007: val_loss did not improve from 1.05203\n",
    "Epoch 8/150\n",
    " - 11s - loss: 0.8341 - acc: 0.6860 - val_loss: 1.0703 - val_acc: 0.5200\n",
    "\n",
    "Epoch 00008: val_loss did not improve from 1.05203\n",
    "Epoch 9/150\n",
    " - 11s - loss: 0.8348 - acc: 0.6860 - val_loss: 1.0834 - val_acc: 0.5200\n",
    "\n",
    "Epoch 00009: val_loss did not improve from 1.05203\n",
    "Epoch 10/150\n",
    " - 11s - loss: 0.8333 - acc: 0.6860 - val_loss: 1.0778 - val_acc: 0.5200\n",
    "\n",
    "Epoch 00010: val_loss did not improve from 1.05203\n",
    "Epoch 11/150\n",
    " - 11s - loss: 0.8334 - acc: 0.6860 - val_loss: 1.0826 - val_acc: 0.5200\n",
    "\n",
    "Epoch 00011: val_loss did not improve from 1.05203\n",
    "Epoch 12/150\n",
    " - 11s - loss: 0.8330 - acc: 0.6860 - val_loss: 1.1381 - val_acc: 0.5200\n",
    "\n",
    "Epoch 00012: val_loss did not improve from 1.05203\n",
    "Epoch 13/150\n",
    " - 11s - loss: 0.8350 - acc: 0.6860 - val_loss: 1.0885 - val_acc: 0.5200\n",
    "\n",
    "Epoch 00013: val_loss did not improve from 1.05203\n",
    "Epoch 14/150\n",
    " - 11s - loss: 0.8323 - acc: 0.6860 - val_loss: 1.1161 - val_acc: 0.5200\n",
    "\n",
    "Epoch 00014: val_loss did not improve from 1.05203\n",
    "Epoch 15/150\n",
    " - 11s - loss: 0.8339 - acc: 0.6860 - val_loss: 1.1278 - val_acc: 0.5200\n",
    "\n",
    "Epoch 00015: val_loss did not improve from 1.05203\n",
    "Epoch 00015: early stopping\n",
    "\n",
    "Test accuracy: 65.5000%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New dataset is small but very different from the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB\n",
    "attrs - (http://www.attrs.org/en/stable/examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "=================================================================\n",
      "Total params: 260,160\n",
      "Trainable params: 260,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:29: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"bl...)`\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, Input, Conv2D, MaxPooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "img_width, img_height = 256, 256\n",
    "train_data_dir = \"./data/train\"\n",
    "validation_data_dir = \"./data/valid\"\n",
    "nb_train_samples = 4216\n",
    "nb_validation_samples = 466\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "### Build the network \n",
    "img_input = Input(shape=(256, 256, 3))\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "# Block 2\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "\n",
    "model = Model(input = img_input, output = x)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 524288)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              536871936 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "=================================================================\n",
      "Total params: 538,181,696\n",
      "Trainable params: 538,181,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:28: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-143ecf0325a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mlayer_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mlayer_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;31m#[layer.name for layer in model.layers]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \"\"\"\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_1 (InputLayer)         (None, 256, 256, 3)       0         \n",
    "_________________________________________________________________\n",
    "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
    "_________________________________________________________________\n",
    "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
    "_________________________________________________________________\n",
    "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
    "_________________________________________________________________\n",
    "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
    "_________________________________________________________________\n",
    "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
    "_________________________________________________________________\n",
    "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
    "=================================================================\n",
    "Total params: 260,160.0\n",
    "Trainable params: 260,160.0\n",
    "Non-trainable params: 0.0\n",
    "\"\"\"\n",
    "\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "#[layer.name for layer in model.layers]\n",
    "\"\"\"\n",
    "['input_1',\n",
    " 'block1_conv1',\n",
    " 'block1_conv2',\n",
    " 'block1_pool',\n",
    " 'block2_conv1',\n",
    " 'block2_conv2',\n",
    " 'block2_pool']\n",
    "\"\"\"\n",
    "\n",
    "import h5py\n",
    "weights_path = 'vgg19_weights.h5' # ('https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5)\n",
    "f = h5py.File(weights_path)\n",
    "\n",
    "list(f[\"model_weights\"].keys())\n",
    "\"\"\"\n",
    "['block1_conv1',\n",
    " 'block1_conv2',\n",
    " 'block1_pool',\n",
    " 'block2_conv1',\n",
    " 'block2_conv2',\n",
    " 'block2_pool',\n",
    " 'block3_conv1',\n",
    " 'block3_conv2',\n",
    " 'block3_conv3',\n",
    " 'block3_conv4',\n",
    " 'block3_pool',\n",
    " 'block4_conv1',\n",
    " 'block4_conv2',\n",
    " 'block4_conv3',\n",
    " 'block4_conv4',\n",
    " 'block4_pool',\n",
    " 'block5_conv1',\n",
    " 'block5_conv2',\n",
    " 'block5_conv3',\n",
    " 'block5_conv4',\n",
    " 'block5_pool',\n",
    " 'dense_1',\n",
    " 'dense_2',\n",
    " 'dense_3',\n",
    " 'dropout_1',\n",
    " 'global_average_pooling2d_1',\n",
    " 'input_1']\n",
    "\"\"\"\n",
    "\n",
    "# list all the layer names which are in the model.\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Here we are extracting model_weights for each and every layer from the .h5 file\n",
    ">>> f[\"model_weights\"][\"block1_conv1\"].attrs[\"weight_names\"]\n",
    "array([b'block1_conv1/kernel:0', b'block1_conv1/bias:0'], \n",
    "      dtype='|S21')\n",
    "# we are assiging this array to weight_names below \n",
    ">>> f[\"model_weights\"][\"block1_conv1\"][\"block1_conv1/kernel:0]\n",
    "<HDF5 dataset \"kernel:0\": shape (3, 3, 3, 64), type \"<f4\">\n",
    "# The list comprehension (weights) stores these two weights and bias of both the layers \n",
    ">>>layer_names.index(\"block1_conv1\")\n",
    "1\n",
    ">>> model.layers[1].set_weights(weights)\n",
    "# This will set the weights for that particular layer.\n",
    "With a for loop we can set_weights for the entire network.\n",
    "\"\"\"\n",
    "for i in layer_dict.keys():\n",
    "    weight_names = f[\"model_weights\"][i].attrs[\"weight_names\"]\n",
    "    weights = [f[\"model_weights\"][i][j] for j in weight_names]\n",
    "    index = layer_names.index(i)\n",
    "    model.layers[index].set_weights(weights)\n",
    "\n",
    "'''\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import glob\n",
    "\n",
    "features = []\n",
    "for i in tqdm(files_location):\n",
    "        im = cv2.imread(i)\n",
    "        im = cv2.resize(cv2.cvtColor(im, cv2.COLOR_BGR2RGB), (256, 256)).astype(np.float32) / 255.0\n",
    "        im = np.expand_dims(im, axis =0)\n",
    "        outcome = model_final.predict(im)\n",
    "        features.append(outcome)\n",
    "\n",
    "'''\n",
    "\n",
    "for layer in model.layers[:2]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "x = model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\", name='dense_1')(x)\n",
    "x = Dropout(0.5, name='dropout_1')(x)\n",
    "x = Dense(1024, activation=\"relu\", name='dense_2')(x)\n",
    "predictions = Dense(3, activation=\"softmax\", name='dense_3')(x)\n",
    "\n",
    "model = Model(input = model.input, output = predictions)\n",
    "\n",
    "model.summary()\n",
    "# compile the model \n",
    "model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "# Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "train_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "batch_size = batch_size, \n",
    "class_mode = \"categorical\")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "validation_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "class_mode = \"categorical\")\n",
    "\n",
    "# Save the model according to the conditions  \n",
    "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "\n",
    "# Train the model \n",
    "model_final.fit_generator(\n",
    "train_generator,\n",
    "samples_per_epoch = nb_train_samples,\n",
    "epochs = epochs,\n",
    "validation_data = validation_generator,\n",
    "nb_val_samples = nb_validation_samples,\n",
    "callbacks = [checkpoint, early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
